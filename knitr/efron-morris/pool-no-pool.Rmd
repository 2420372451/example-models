---
title: "Hierarchical Partial Pooling for Repeated Binary Trials"
author: "Bob Carpenter"
date: "4 January 2016"
output: 
  html_document: 
    theme: readable
---

### Abstract

This note illustrates the effects on posterior inference of pooling
data (aka sharing strength) across items for repeated binary trial
data.  It provides Stan models and R code to fit and check predictive
models for three situations: (a) complete pooling, which assumes each
item is the same, (b) no pooling, which assumes the items are
unrelated, and (c) partial pooling, where the similarity among the
items is estimated.  We consider two hierarchical models to estimate
the partial pooling, one with a beta prior on chance of success and
another with a normal prior on the log odds of success.  The note
explains with working examples how to (i) fit models in RStan and plot
the results in R ussing ggplot2, (ii) estimate event probabilities,
(iii) evaluate posterior predictive densities to compare model
predictions, (iv) rank items by chance of success, (v) perform
multiple comparisons in several settings, (vi) replicate new data for
posterior p-values, and (vii) perform graphical posterior predictive
checks.

Efron and Morris's (1975) baseball batting average data is used as a
running example, but three additional data sets are provided in
ready-to-use format: tumors in control groups of rats across
experiments, mortality in infant surgeries across hospitals, and an
entire season of baseball hits across players.

## Repeated Binary Trials

Suppose that for each of $N$ items, we observe $y_n$ successes out of
$K_n$ trials.  For example, the data may consist of

* rat tumor development, with $y_n$ rats developing tumors of $K_n$ total
  rats in experimental control group $n \in 1{:}N$ (Gelman et al. 2013)

* surgical mortality, with $y_n$ surgical patients dying in $K_n$ surgeries
  for hospitals $n \in 1{:}N$ (Spiegelhalter et al. 1996)

* baseball batting ability, with $y_n$ hits in $K_n$ at bats for 
  baseball players $n \in 1{:}N$ (Efron and Morris 1975)

* machine learning system accuracy, with $y_n$ correct classifications
  out of $K_n$ examples for systems $n \in 1{:}N$ (any ML conference
  proceedings; Kaggle competitions)

#### Baseball Hits (Efron and Morris 1975)

We include the data from Table 1 of (Efron and Morris 1975) as
<code>efron-morris-75-data.tsv</code> (it was downloaded 24 Dec 2015
from
[here](http://www.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt)).
It is drawn from the 1970 Major League Baseball season from both
leagues.

```{r comment=NA}
df <- read.csv("efron-morris-75-data.tsv", sep="\t");
df <- data.frame(FirstName = df$FirstName,
                 LastName = df$LastName,
                 Hits = df$Hits,
                 At.Bats = df$At.Bats,
                 RemainingAt.Bats = df$RemainingAt.Bats,
                 RemainingHits = df$SeasonHits - df$Hits);
print(df);
```

We will only need a few columns of the data; we will be using the
remaining hits and at bats to evaluate the pooling performed by the
models.

```{r comment=NA}
N <- dim(df)[1]
K <- df$At.Bats
y <- df$Hits
K_new <- df$RemainingAt.Bats;
y_new <- df$RemainingHits;
```

The data separates the outcome from the initial 45 at-bats from the
rest of the season.  After running this code, <code>N</code> is the
number of items (players).  Then for each item <code>n</code>,
<code>K[n]</code> is the number of initial trials (at-bats),
<code>y[n]</code> is the number of initial successes (hits),
<code>K_new[n]</code> is the remaining number of trials (remaining
at-bats), and <code>y_new[n]</code> is the number of successes in the
remaining trials (remaining hits).

Although we consider many models, the data is coded as follows for all
of them.  

```
data {
  int<lower=0> N;           // items
  int<lower=0> K[N];        // initial trials
  int<lower=0> y[N];        // initial successes

  int<lower=0> K_new[N];    // new trials
  int<lower=0> y_new[N];    // new successes
}
```

As usual, we follow the covnention of naming our program
variables after the variables we use when we write the model out
mathematically in a paper.  We also choose capital letters for integer
constants and y for the main observed variable(s).

## Pooling

With *complete pooling*, each item is assumed to have the same chance
of success.  With *no pooling*, each item is assumed to have a
completely unrelated chance of success.  With *partial pooling*, each
item is assumed to have a different chance of success, but the data
for all of the observed items informs the estimates for each item.  
Partial pooling with infinite population variance is the same as no
pooling;  partial pooling with zero population variance leads to
complete pooling.

In a hierarchical model, the population of items is directly modeled,
typically with a mean chance of success for the population and a scale
of abilities modeling the average member and amount of variation
within a population.

In the following sections, all three types of pooling models will be
fit for the baseball data.


## Model 1: Complete Pooling

The complete pooling model assumes a single parameter $\phi$
representing the chance of success for all items.  It is necessary in
Stan to declare parameters with constraints corresponding to their
support in the model.  Because $\phi$ will be used as a binomial
parameter, we must have $\phi \in (0,1)$.  The variable
<code>phi</code> must therefore be declared in Stan with the following
lower- and upper-bound constraints.

```
parameters {
  real<lower=0, upper=1> phi;  // chance of success (pooled)
}
```

The consequences for leaving the constraint off is that the program
may fail during random initialization or during an iteration because
it will try values for <code>phi</code> outside of $(0,1)$.

Assuming each player's at-bats are independent Bernoulli trials, the
sampling distribution for each player's number of hits $y_n$ is modeled as

\[
p(y_n \, | \, \phi) 
\ = \
\mathsf{Binomial}(y_n \, | \, K, \phi).
\]

When viewed as a function of $\phi$ for fixed $y_n$, this is called
the likelihood function.  The proportional sign $f(x) \propto g(x)$ means
there is some constant $c$ such that $f(x) = c \, g(x)$.  We consider $K$
and $y_n$ constants here, so the proportionality is considering the
likelihood function.

Assuming each player is independent leads to the complete data
likelihood

\[
p(y \, | \, \phi) = \prod_{n=1}^N \mathsf{Binomial}(y_n \, | \, K, \phi).
\]

We will assume a uniform prior on $\phi$,

\[
p(\phi) 
\ = \
\mathsf{Uniform}(\phi \, | \, 0, 1) 
\ = \
1.
\]

Whether a prior is uniform or not depends on the scale with which the
parameter is expressed.  Here, the variable $\phi$ is a chance of
success in $(0, 1)$.  If we were to consider the log-odds of success,
$\log \frac{\phi}{1 - \phi}$, a uniform prior on log-odds is not the
same as a uniform prior on chance of success (they are off by the
Jacobian of the transform).  A uniform prior on chance of success
translates to a unit logistic prior on the log odds (and in fact, the
definition of the unit logistic density can be derived by calculating
the Jacobian of the transform).

By default, Stan places a uniform prior over the values meeting the
constraints on a parameter.  Because <code>phi</code> is constrained
to fall in $(0,1)$, there is no need to explicitly specify the uniform
prior on $\phi$.  

The likelihood is expressed as a vectorized sampling statement in Stan
as

```
model {
  y ~ binomial(K, phi);
}
```

The vectorized sampling statement above is equivalent to but more
efficient than the following explicit loop.

```
  for (n in 1:N)  
    y[n] ~ binomial(K[n], phi);
```

In general, Stan will match dimensions, repeating scalars as
necessary; any vector or array arguments must be the same size.  When
used as a function, the result is the sum of the log densities.  The
vectorized form can be up to an order of magnitude faster in some
cases, depending on how many repeated calculations can be avoided.

The actual Stan program in <code>pool.stan</code> has many more
derived quantities that will be used in the rest of this note; see
the appendix for the full code of all of the models discussed.

We start by loading the RStan package.

```{r comment=NA}
library(rstan);
```

The model can be fit as follows, reading the data out of the
environment by name (the output result of running Stan is hidden);
normally we would prefer to encapsulate the data in a list to avoid
naming conflicts in the top-level namespace.

```{r results='hide', comment=NA}
M <- 10000;
fit_pool <- stan("pool.stan", data=c("N", "K", "y", "K_new", "y_new"),
                 iter=(M / 2), chains=4);
ss_pool <- extract(fit_pool);
```



The posterior sample for <code>phi</code> can be summarized as
follows.


```{r comment=NA}
print(fit_pool, c("phi"), probs=c(0.1, 0.5, 0.9));
```

The summary statistics begin with the posterior mean, the MCMC
standard error on the posterior mean, and the posterior standard
deviation.  Then there are 0.1, 0.5, and 0.9 quantiles, which provide
the posterior median and boundaries of the central 80% interval.  The
last two columns are for the effective sample size (MCMC standard
error is the posterior standard deviation divided by the square root
of the effective sample size) and the $\hat{R}$ convergence diagnostic
(its value will be 1 if the chains have all converged to the same
posterior mean and variance; see the Stan Manual or Gelman et
al. 2013).  The $\hat{R}$ value here is consistent with convergence
and the effective sample size is good (roughly half the number of
posterior draws; by default Stan uses as many iterations to warmup as
it does for drawing the sample).

The result is a posterior mean for $\theta$ of $0.27$ with an
80% central posterior interval of $(0.25, 0.29)$.

We know from historical data that many players exceed a 0.30 chance of
success over their careers with thousands of at-bats, so we have
reason to believe the complete pooling results are too conservative.


## Model 2: No Pooling

A model with no pooling involves a separate chance-of-success
parameter $\theta_n \in (0,1)$ for each item $n$.

The prior on each $\theta_n$ is uniform,

\[
p(\theta_n) = \mathsf{Uniform}(\theta_n \, | \, 0,1),
\]

and the $\theta_n$ are assumed to be independent, 

\[
p(\theta) = \prod_{n=1}^N \mathsf{Uniform}(\theta_n \, | \, 0,1).
\]

The likelihood then uses the chance of success $\theta_n$ for item $n$
in modeling the number of successes $y_n$ as

\[
p(y_n \, | \, \theta_n) = \mathsf{Binomial}(y_n \, | \, K, \theta_n).
\]

Assuming the $y_n$ are independent (conditional on $\theta$), this
leads to the total data likelihood

\[
p(y \, | \, \theta) = \prod_{n=1}^N \mathsf{Binomial}(y_n \, | \, K, \theta_n).
\]

The Stan program for no pooling only differs in declaring the ability
parameters as an $N$-vector rather than a scalar.

```
parameters {
  vector<lower=0, upper=1>[N] theta; // chance of success
}
```

The constraint applies to each <code>theta[n]</code> and implies an
independent uniform prior on each.

The model block defines the likelihood as binomial, using the
efficient vectorized form

```
model {
  y ~ binomial(K, theta);  // likelihood
}
```

This is equivalent to the less efficient looped form

```
  for (n in 1:N)
    y[n] ~ binomial(K[n], theta[n]);
```

The full Stan program with all of the extra generated quantities, is
in <code>no-pool.stan</code>, which is shown in the appendix.

This model can be fit the same way as the last model.

```{r  comment=NA, results='hide'}
fit_no_pool <- stan("no-pool.stan", data=c("N", "K", "y", "K_new", "y_new"),
                    iter=(M / 2), chains=4);
ss_no_pool <- extract(fit_no_pool);
```

Results are displayed the same way.

```{r comment=NA}
print(fit_no_pool, c("theta"), probs=c(0.1, 0.5, 0.9));
```

Now there is a separate line for each item's estimated $\theta_n$.
The posterior mode is the maximum likelihood estimate, but that
requires running Stan's optimizer to find; the posterior mean and
median will be reasonably close to the posterior mode despite the
skewness (the posterior can be shown analytically to be a Beta
distribution).

Each 80% interval is much wider than the estimated interval for the
population in the complete pooling model; this is to be
expected---there are only 45 data items for each parameter here as
opposed to 810 in the complete pooling case.  Also, as the estimated
chance of success goes up toward 0.5, the 80% intervals gets wider;
this is to be expected for chance of success parameters (the standard
deviation of $\mathsf{Binomial}(K, \theta)$ is $\sqrt{\frac{\theta \,
(1 - \theta)}{K}}$.  If the items each had different numbers of
trials, the intervals would also vary based on size.
  
The no-pooling model model provides better MCMC mixing than the
pooling model as indicated by the effective sample size and
convergence diagnostics; although not in and of itself meaningful, it
is often the case that badly misspecified models provide difficult
computationally (a result Andrew Gelman has dubbed "[The Folk
Theorem](http://andrewgelman.com/2009/05/24/handy_statistic/)").

Based on our existing knowledge of baseball, the no-pooling model is
almost certainly overestimating the high abilities and underestimating
lower abilities (Ted Williams, 30 years prior to the year this data
was collected, was the last player with a success rate of 40%, whereas
20% is too low for all but a few rare defensive specialists).

## Model 3: Partial Pooling (Chance of Success)

Complete pooling provides estimated abilities that are too narrow and
removes any chance of modeling population variation.  Estimating each
chance of success separately wtihout any pooling provides estimated
abilities that are too broad and hence too variable.  Clearly some
amount of pooling between these two extremes is called for.  But how
much?

A hierarchical model treats the players as belonging to a population
of players.  The properties of this population will be estimated along
with player abilities, implicitly controlling the amount of pooling
that is applied.  The more variable the (estimate of the) population,
the less pooling is applied.

Mathematically, the hierarchical model places a prior on the abilities
with parameters that are themselves estimated.  In this case, we will
assume a <a
href="https://en.wikipedia.org/wiki/Beta_distribution">beta
distribution</a> as the prior as it is scaled to values in $(0, 1)$,

\[
p(\theta_n \, | \, \alpha, \beta)
\ = \ \mathsf{Beta}(\theta_n \, | \, \alpha, \beta),
\]

where $\alpha, \beta > 0$ are the parameters of the prior, and
$\Gamma(u)$ is the [gamma
function](https://en.wikipedia.org/wiki/Gamma_function).  In this
simple case of a conjugate prior, the prior's parameters can be
interpreted as prior data, with $\alpha - 1$ being the prior number of
successes and $\beta - 1$ being the prior number of failures, and
$\alpha = \beta = 1$ corresponding to no prior observations and thus a
uniform distribution.  Each $\theta_n$ will be modeled as
conditionally independent given the prior parameters, so that the
complete prior is

\[
p(\theta \, | \, \alpha, \beta) 
= \prod_{n=1}^N \mathsf{Beta}(\theta_n \, | \, \alpha, \beta).
\]

The parameters $\alpha$ and $\beta$ are themselves given priors
(sometimes called hyperpriors).  Rather than parameterize $\alpha$ and
$\beta$ directly, we will instead use put priors on $\phi \in (0, 1)$
and $\kappa > 0$, and then define

\[
\alpha = \kappa \, \phi
\]

and

\[
\beta = \kappa \, (1 - \phi).
\]

This reparameterization is convenient because 

* $\phi = \frac{\alpha}{\alpha + \beta}$ is the mean of
$\mathsf{Beta}(\alpha, \beta)$, and

* $\kappa = \alpha + \beta$ is the prior count plus two (roughly
inversely related to the variance).

We will follow Gelman et al. (2013, Chapter 5) in providing a prior
that factors into a uniform prior on $\phi$,

\[
p(\phi) = \mathsf{Uniform}(\phi \, | \, 0, 1), 
\]

and a Pareto prior on $\kappa$,

\[
p(\kappa) = \mathsf{Pareto}(\kappa \, | \, 1, 1.5) \propto \kappa^{-2.5}.
\]

with the restriction $\kappa > 1$.  There must be some bound on
$\kappa > 0$ in order for the Pareto distribution to be properly
normalized, and $\kappa > 1$ is a conservative choice for this
problem.  The constraint $\kappa > 1$ must therefore be included in
the Stan parameter declaration, because Stan programs require support
on the parameter values that satisfy their declared constraints.

The Stan code follows the definitions, with parameters declared with
appropriate constraints as follows.

```
parameters {
  real<lower=0, upper=1> phi;         // population chance of success
  real<lower=1> kappa;                // population concentration
  vector<lower=0, upper=1>[N] theta;  // chance of success 
}
```

The lower-bound on $\kappa$ matches the first argument to the Pareto
distribution in the model block.

```
model {
  kappa ~ pareto(1, 1.5);                        // hyperprior
  theta ~ beta(phi * kappa, (1 - phi) * kappa);  // prior
  y ~ binomial(K, theta);                        // likelihood
}
```

The values of $\alpha = \phi \, \kappa$ and $\beta = (1 - \phi) \,
\kappa$ are computed in the arguments to the vectorized Beta sampling
statment.  The prior on $\phi$ is implicitly uniform because it is
explicitly constrained to line in $(0, 1)$.  The prior on $\kappa$ is
coded following the model definition. 

The full model with all generated quantities can coded in Stan as in
the file <code>hier.stan</code>; it is displayed in the appendix.

Fitting in this case adds control parameters which set the initial
step size lower than the default and the target acceptance rate
(<code>adapt_delta</code>) higher than the default values; this causes
smaller step sizes and helps with arithmetic stability in hierarchical
models such as this one (with centered parameterizations and small
data sizes).

```{r  comment=NA, results='hide'}
fit_hier <- stan("hier.stan", data=c("N", "K", "y", "K_new", "y_new"),
                 iter=(M / 2), chains=4, 
                 control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier <- extract(fit_hier);
```

Summary statistics for the posterior are printed as before.

```{r comment=NA}
print(fit_hier, c("theta", "kappa", "phi"), probs=c(0.1, 0.5, 0.9));
```

Because the Beta prior is conjugate to the binomial likelihood, the
amount of interpolation between the data and the prior in this
particular case is easy to quantify.  The data consists of $K$
observations.  The prior will be weighted as if it were $\kappa - 2$
observations (specifically $\phi \, \kappa - 1$ prior successes and
$(1 - \phi) \, \kappa - 1$ prior failures).

The parameter $\kappa$ is not well determined by the combination of
data and Pareto prior, with a posterior 80% interval of roughly $(25,
225)$.  By the informal discussion above, $\kappa \in (25, 225)$
ranges from weighting the data 2:1 relative to the prior to weighting
it 1:5.  The wide posterior interval for $\kappa$ arises because the
exact variance in the population is not well constrained by only 18
trials of size 45.  If there were more items (higher $N$) or even more
trials per item (higher $K$), the posterior for $\kappa$ would be more
tightly constrained (see the exercises for a comparison).

It is also evident from the posterior summary that the lower effective
sample size for $\kappa$ indicates it is not mixing as well as the
other components of the model; again, this is to be expected with a
centered hierarchical prior and low data counts.  This is an example
where a poorly constrained parameter leads to reduced computational
efficiency (as reflected in the effective sample size).  Such poor
mixing is typical of centered parameterizations in hierarchical models
(Betancourt and Girolami 2015).  In the next section, we address this
problem with an alternative hierarchical prior; it may be possible to
create a non-centered parameterization of the Beta prior, but I don't
know how to do it.

Figure 5.3 from (Gelman et al. 2014) plots the fitted values for
$\phi$ and $\kappa$ on the unconstrained scale, which is the space
over which Stan is sampling.  The variable $\phi \in (0,1)$
istransformed to $\mathrm{logit}(\phi) = \log(\phi / (1 - \phi))$ and
$\kappa \in (0, \infty)$ is transformed to $\log \kappa$.  We
reproduce that figure here for our running example.

```{r}
phi_sim <- ss_hier$phi;
kappa_sim <- ss_hier$kappa;
df_bda3_fig_5_3 <- data.frame(x = log(phi_sim / (1 - phi_sim)),
                              y = log(kappa_sim));
library(ggplot2);
plot_bda3_fig_5_3 <- 
  ggplot(df_bda3_fig_5_3, aes(x=x, y=y)) +
  geom_point(shape=19, alpha=0.15) +
  xlab("logit(phi) = log(alpha / beta)") +
  ylab("log(kappa) = log(alpha + beta)");
plot_bda3_fig_5_3;
```

The (upside-down) funnel-like shape of the posterior is evident, with
higher $\kappa$ values corresponding to lower variance (hence the
upside-downedness) and thus a narrower range of $\phi$ values; this
phenomenon is discussed in the *Stan Reference Manual* and by
Betancourt and Girolami (2015).


## Model 4: Partial Pooling (Log Odds)

The previous models all used a direct parameterization of the
chance-of-success $\theta_n \in (0,1)$.  In this section, we consider
an alternative parameterization in terms of the log-odds $\alpha_n$, which are
defined by the logit transform as

\[
\alpha_n 
= \mathrm{logit}(\theta_n) 
= \log \, \frac{\theta_n}{1 - \theta_n}.
\]

For example, $\theta_n = 0.25$ corresponds to odds of $.25$ to $.75$
(equivalently, $1$ to $3$), or log-odds of $\log .25 / .75 = -1.1$.

We will still use a binomial likelihood, only now we have logit as a
so-called "link" function.

\[
p(y_n \, | \, K, \alpha_n) 
\ = \ \mathsf{Binomial}(y_n \, | \, K, \mathrm{logit}^{-1}(\alpha_n))
\]

The inverse logit function is the logistic sigmoid from which logistic
regression gets its name,

\[
\mathrm{logit}^{-1}(\alpha_n) = \frac{1}{1 + \exp(-\alpha_n)} = \theta_n.
\]

By construction, for any $\alpha_n \in (-\infty, \infty)$,
$\mathrm{logit}^{-1}(\alpha_n) \in (0, 1)$; the sigmoid converts
arbitrary log odds back to the chance-of-success scale.

Stan has a binomial probability function with a built-in logit link
function, with wich we can define the likelihood directly as

\[
p(y_n \, | \, K, \alpha_n) 
\ = \ \mathsf{BinomialLogit}(y_n \, | \, K, \alpha_n)
\ = \ \mathsf{Binomial}(y_n \, | \, K, \mathrm{logit}^{-1}(\alpha_n)).
\]

We use a simple normal hierarchical prior,

\[
p(\alpha_n \, | \, \mu, \sigma)
= \mathsf{Normal}(\alpha_n \, | \, \mu, \sigma).
\]

Then one level up, we use a weakly informative hyperprior for $\mu$,

\[
p(\mu) = \mathsf{Normal}(\mu \, | \, -1, 1),
\]

which places 95% of the prior probability for $\mu$ in the interval
$(-3, 1)$, which inverse-logit transforms to the interval $(0.05,
0.73)$ with a median 0.27 chance of success.  An even narrower prior
is actually motivated here from substantial baseball knowledge.  This
value should obviously be changed for other applications.

The prior scale $\sigma$ can be taken to be a half-normal.  

\[
p(\sigma) = 0.5 \, \mathsf{Normal}(\sigma \, | \, 0, 1)
\]

This is a fairly broad prior here, being on the log-odds scale. 

One of the major advantages of casting the problem in terms of log
odds is that its now easier to add in fixed effects and other
multilevel effects, or even varying intercepts and slopes with
multivariate priors (see Gelman and Hill (2007) for a range of
examples of mixed effects models).

#### Non-Centered Parameterization

Betancourt and Girolami (2015) provide a detailed discussion of why
the centered parameterization used above is problematic for
hierarchical models with small counts per group (here, the 45 initial
at bats for each player).  To mitigate the problem, they suggest
moving to a non-centered parameterization, as has also been shown to
be helpful for Gibbs and random-walk Metropolis samplers.  This
basically amounts to changing the parameterization over which sampling
is done, taking now a standard unit normal prior for a new variable,

\[
\alpha^{\mathrm{std}}_n = \frac{\alpha_n - \mu}{\sigma}.
\]

Then we can parameterize in terms of $\alpha^{\mathrm{std}}$, which
has a standard-normal distribution

\[
p(\alpha^{\mathrm{std}}_n) = \mathsf{Normal}(\alpha^{\mathrm{std}}_n \, | \, 0, 1).
\]

We can then define our original $\alpha$ as a derived quantity

\[
\alpha_n = \mu + \sigma \, \alpha^{\mathrm{std}}_n.
\]

We code this implicitly in the Stan model by defining the
likelihood as

\[
p(y_n \, | \, \alpha^{\mathrm{std}}_n, \mu, \sigma, K)
\ = \
\mathsf{BinomialLogit}(K_n, \mu + \sigma \, \alpha_n).
\]

This decouples the sampling distribution for $\alpha^{\mathrm{std}}$
from $\mu$ and $\sigma$, greatly reducing their covariance, converting
their funnel-like shape to more of a circle.  For comparison, we plot
the posterior here.

```{r}
mu_sim <- ss_hier_logit$mu;
sigma_sim <- ss_hier_logit$sigma;
df_bda3_fig_5_3_logit <- data.frame(x = mu_sim, 
                                    y = log(sigma_sim));
plot_bda3_fig_5_3_logit <- 
  ggplot(df_bda3_fig_5_3_logit, aes(x=x, y=y)) +
  geom_point(shape=19, alpha=0.15) +
  xlab("mu") + 
  ylab("log sigma")
plot_bda3_fig_5_3_logit;
```

<b>[FIXME: Still looks like a funnel.  What's up with that?  Is it
because the data's consistent with $\sigma=0$?]</b>

The Stan program's parameter declaration and model directly follow the
definition.

```
parameters {
  real mu;                       // population mean of success log-odds
  real<lower=0> sigma;           // population sd of success log-odds
  vector[N] alpha_std;           // success log-odds
}
model {
  mu ~ normal(-1, 1);                             // hyperprior
  sigma ~ normal(0, 1);                           // hyperprior
  alpha_std ~ normal(0, 1);                       // prior
  y ~ binomial_logit(K, mu + sigma * alpha_std);  // likelihood
}
```

Because the parameters to the prior for $\sigma$ are constants, the
normalization for the half-prior (compared to the full prior) is
constant and does not need to be included in the notation.  This only
works if the parameters to the density are data or constants; if they
are defined as parameters or as quantities depending on parameters,
then explicit truncation is required.

For the purposes of comparison, the chance of success $\theta$ is
computed as a generated quantity.

```
generated quantities {
  vector[N] theta;  // chance of success
  ...
  for (n in 1:N)
    theta[n] <- inv_logit(mu + sigma * alpha_std[n]);
  ...
}
```

The full Stan program for the hierarchical logistic model is in
<code>hier-logit.stan</code> and displayed in the appendix.

It is fit and the values are extracted as follows.

```{r  comment=NA, results='hide'}
fit_hier_logit <- stan("hier-logit.stan", data=c("N", "K", "y", "K_new", "y_new"),
                       iter=(M / 2), chains=4,
                       control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier_logit <- extract(fit_hier_logit);
```

We can print as before.

```{r comment=NA}
print(fit_hier_logit, c("alpha_std", "theta", "mu", "sigma"), probs=c(0.1, 0.5, 0.9));
```

It is clear from the wide posteriors for the $\theta_n$ that there is
considerable uncertainty in the estimates of chance-of-success on an
item-by-item basis.  With an 80% interval of $(0.03, 0.32)$, it is
clear that the data is consistent with complete pooling (i.e. $\sigma
= 0$).

Compared to the direct beta priors with uniform and Pareto hyperpriors
shown in the first example, the normal prior on log odds exerts more
pull toward the population mean.  The posterior means for $\theta$ ranged from
0.22 to 0.32 with the beta prior, but only range from 0.24 to 0.29 for
the normal prior.  Furthermore, the posterior intervals for each
values are shrunk compared to the beta prior.  For example, Roberto
Clemente ($n = 1$), has an 80% central posterior interval of $(0.25,
0.35)$ in the logistic model, whereas he had an 80% posterior interval
of $(.26, .39)$ with a hierarchical beta prior.


## Observed vs. Estimated Chance of Success

Figure 5.4 from (Gelman et al. 2013) plots the observed number of
successes $y_n$ for the first $K_n$ trials versus the median and 80\%
intervals for the estimated chance-of-success parameters $\theta_n$ in
the posterior.  The following R code reproduces a similar plot for our data.

```{r}
ss_quantile <- function(ss, N, q) {
  result <- rep(NA, N);
  for (n in 1:N) {
    result[n] <- sort(ss$theta[,n])[M * q];
  }
  return(result);
}

theta_10_pool <- ss_quantile(ss_pool, N, 0.1);
theta_50_pool <- ss_quantile(ss_pool, N, 0.5);
theta_90_pool <- ss_quantile(ss_pool, N, 0.9);

theta_10_no_pool <- ss_quantile(ss_no_pool, N, 0.1);
theta_50_no_pool <- ss_quantile(ss_no_pool, N, 0.5);
theta_90_no_pool <- ss_quantile(ss_no_pool, N, 0.9);

theta_10_hier <- ss_quantile(ss_hier, N, 0.1);
theta_50_hier <- ss_quantile(ss_hier, N, 0.5);
theta_90_hier <- ss_quantile(ss_hier, N, 0.9);

theta_10_hier_logit <- ss_quantile(ss_hier_logit, N, 0.1);
theta_50_hier_logit <- ss_quantile(ss_hier_logit, N, 0.5);
theta_90_hier_logit <- ss_quantile(ss_hier_logit, N, 0.9);

pop_mean <- sum(y) / sum(K);

df_plot2 <- data.frame(x = rep(y / K, 4),
                       y = c(theta_50_pool, theta_50_no_pool,
                             theta_50_hier, theta_50_hier_logit),
                       model = c(rep("complete pooling", N),
                                 rep("no pooling", N),
                                 rep("partial pooling", N),
				 rep("partial pooling (log odds)", N)));

plot_bda3_fig_5_4 <-
  ggplot(df_plot2, aes(x=x, y=y)) +
  geom_hline(aes(yintercept=pop_mean), colour="lightpink") +
  geom_abline(intercept=0, slope=1, colour="skyblue") +
  facet_grid(. ~ model) +
  geom_errorbar(aes(ymin=c(theta_10_pool, theta_10_no_pool, theta_10_hier, theta_10_hier_logit),
                    ymax=c(theta_90_pool, theta_90_no_pool, theta_90_hier, theta_90_hier_logit)),
                width=0.005, colour="gray60") +
  geom_point(colour="gray30", size=0.75) +
  coord_fixed() +
  scale_x_continuous(breaks = c(0.2, 0.3, 0.4)) +
  xlab("observed rate, y[n] / K[n]") +
  ylab("chance of success, theta[n]");
plot_bda3_fig_5_4;
```

The horizontal axis is the observed rate of success, broken out by
player (the overplotting is from players with the same number of
successes---they all had the same number of trials in this data).  The
dots are the posterior medians with bars extending to cover the
central 80% posterior interval.  Players with the same observed rates
are indistinguishable, any differences in estimates are due to MCMC
error.

The horizontal red line has an intercept equal to the overall success rate,

\[
\frac{\sum_{n=1}^N y_n}{\sum_{n=1}^N K_n}
\ = \
\frac{215}{810}
\ = \ 
0.266.
\]

The overall success rate is also the posterior mode (i.e., maximum
likelihood estimate) for the complete pooling model.

The diagonal blue line has intercept 0 and slope 1.  Estimates falling
on this line make up the maximum likelihood estimates for the
no-pooling model.

The choice of likelihood function and parameterization (a single
parameter or one parameter per item) makes a huge impact here on the
estimated chance of success for each item.  This is true even in
non-Bayesian settings---the points are very close to where the maximum
likelihood estimates will be and these vary quite dramatically between
the complete pooling and no pooling extremes.  It is important to keep
in mind that it's not just the choice of prior that contributes to the
"subjective" modeling of a data set; the choice of likelihood is
equally if not more sensitive to modeling assumptions.

In the no pooling and complete pooling cases, the posterior can be
calculated analytically and shown to be a beta distribution.  The
median of a beta distribution (plotted points in the complete pooling
and no pooling cases) is greater than the mode of a beta distribution
(the blue line, corresponding to the MLE); this assumes $\alpha, \beta
> 1$ (see the [Wikipedia article on the beta
distribution](https://en.wikipedia.org/wiki/Beta_distribution#Mean.2C_mode_and_median_relationship)
for an explanation).  The reason this is not evident in the complete
pooling case is that there is far more data, and as $N \rightarrow
\infty$, the mean, median, and mode of the beta distribution converge
to the same value.

Plotting the transformed $\theta$ values in the logit-based model,
there is a clear skew in the posteriors for each parameter away from
the population mean.  Overall, the plot makes the amount of pooling
toward the prior evident.


## Posterior Predictive Distribution

Given data $y$ and a model with parameters $\theta$, the posterior
predictive distribution for new data $\tilde{y}$ is

\[
p(\tilde{y} \, | \, y)
\ = \
\int_{\Theta} p(\tilde{y} \, | \, \theta) \ p(\theta \, | \, y) \ \mathrm{d}\theta,
\]

where $\Theta$ is the support of the parameters $\theta$.  What an
integral of this form says is that $p(\tilde{y} \, | \, y)$ is defined
as a weighted average over the legal parameter values $\Theta$ of the
likelihood function $p(\tilde{y} \, | \, \theta)$, with weights given by
the posterior, $p(\theta \, | \, y)$.

We can also define the weighted average of the log likelihood function
over the posterior,
 
\[
\int_{\Theta} \left( \, \log p(\tilde{y} \, | \, \theta) \, \right) \ p(\theta \, | \, y) \ \mathrm{d}\theta.
\]

Note that this is not the same as $\log p(\tilde{y} \, | \, y)$,
because logs do not distribute through integrals;  the best we can do
is get a bound through Jensen's inequality.

#### Evaluating Held-Out Data Predictions

Because the posterior predictive log density is formulated as an
expectation over the posterior, it is straightforward to compute via
MCMC.  Suppose we have new data $y^{\mathrm{new}}$ and want to
estimate its log density.  With $M$ draws $\theta^{(m)}$ from the
posterior $p(\theta \, | \, y)$, the expected posterior log density is
computed by 

\[
\frac{1}{M} \, \sum_{m=1}^M \log p(y^{\mathrm{new}} \, | \, \theta^{(m)}).
\]

#### Simulating Replicated Data

It is also straightforward to use forward simulation from the data
sampling distribution $p(y \, | \, \theta)$ to generate replicated
data $y^{\mathrm{rep}}$ according to the posterior predictive
distribution.  (Recall that $p(y \, | \, \theta)$ is called the sampling
distribution when $\theta$ is fixed and the likelihood when $y$ is fixed.)

With $M$ draws $\theta^{(m)}$ from the posterior $p(\theta \, | \,
y)$, replicated data can be simulated by drawing a sequence of $M$
simulations $y^{\mathrm{rep} \ (m)}$ according to the sampling
distribution $p(y \, | \, \theta^{(m)})$.  This latter computation can
usually be done efficiently (both computationally and statistically)
by means of forward simulation from the sampling distribution.



## Prediction for New Trials

Efron and Morris's (1975) baseball data includes not only the observed
hit rate in the initial 45 at bats, but also includes the data for how
the player did for the rest of the season.  The question arises as to
how well these models predict a player's performance for the rest of
the season based on their initial 45 at bats.

The variables <code>K_new[n]</code> and <code>y_new[n]</code> hold the
number of at bats (trials) trials and the number of hits (successes)
for player (item) <code>n</code>.  To code the evaluation of the held
out data in Stan, we declare a generated quantity to hold the result
and then define it as the sum of the log densities of the new
observations.

```
generated quantities {
  ...
  real log_p_new;  // posterior prediction log density remaining trials
  ...
  log_p_new <- 0;
  for (n in 1:N)
    log_p_new <- log_p_new + binomial_log(y_new[n], K_new[n], theta[n]);
  ...
}
```

This means that the posterior mean for <code>log_p_new</code> will be
the expectation of the log density in the posterior,

\[
\sum_{n=1}^N \log p(y^{\mathrm{new}}_n \, | \, \theta^{(m)}_n).
\]

This calculation is included in all four of the models we have
previously fit and can be displayed directly as follows.

```{r comment=NA}
print(sprintf("%10s  %16s", "MODEL", "log p(y_new | y)"), quote = FALSE);
print(sprintf("%10s  %16.0f", "pool", mean(ss_pool$log_p_new)), quote=FALSE);
print(sprintf("%10s  %16.0f", "no pool", mean(ss_no_pool$log_p_new)), quote=FALSE);
print(sprintf("%10s  %16.0f", "hier", mean(ss_hier$log_p_new)), quote=FALSE);
print(sprintf("%10s  %16.0f", "hier logit", mean(ss_hier_logit$log_p_new)), quote=FALSE);
```

From a predictive standpoint, the models are ranked by the amount of
pooling they do, with complete pooling being the best, and no pooling
being the worst predictively. All of these models do predictions by
averaging over their posteriors, with the amount of posterior
uncertainty also being ranked in reverse order of the amount of
pooling they do.

## Predicting New Observations

We showed above that it is straightforward to generate draws from the
posterior predictive distribution.  With that capability, we can
either generate predictions for new data or we can replicate the data
we already have.  Here we let $z_n$ be distributed according to the
predictive posterior for number of trials $K^{\mathrm{new}}_n$.

The posterior predictions can be declared in Stan as generated
quantities.  Their values are determined by calling the binomial
pseudorandom number generator, which corresponds to the binomial
sampling distribution (likelihood) in this case.

```
generated quantities {
  ...
  int<lower=0> z[N];  // posterior prediction remaining trials
  ...
  for (n in 1:N)
    z[n] <- binomial_rng(K_new[n], theta[n]);
  ...
}
```

It might seem tempting to set $z_n^{(m)}$ to its expectation
$\theta_n^{(m)} \, K^{\mathrm{new}}$ at each iteration rather than
simulating a new value.  The resulting values would suffice for
estimating the posterior mean, but would not capture the uncertainty
in the prediction for $y^{\mathrm{new}}_n$ and would thus not be
useful in estimating predictive standard deviations or quantiles or
as the basis for decision making under uncertainty.

These can be shown in all of our models as follows; the number of
remaining at-bats $K^{\mathrm{new}}$ was printed out in the original
table along with the actual number of hits.

```{r comment=NA}
print(fit_pool, c("z"), probs=c(0.1, 0.5, 0.9), digits=0);
```

For the remaining models, only a selection of <code>z</code> values
are printed.

```{r comment=NA}
print(fit_no_pool, c("z[1]", "z[8]", "z[18]"), probs=c(0.1, 0.5, 0.9), digits=0);
print(fit_hier, c("z[1]", "z[8]", "z[18]"), probs=c(0.1, 0.5, 0.9), digits=0);
print(fit_hier_logit, c("z[1]", "z[8]", "z[18]"), probs=c(0.1, 0.5, 0.9), digits=0);
```

The posterior uncertainty in the simulated $z_n$ is even broader than
for $\theta_n$ because of the additional uncertainty from the
binomial.  For instance, Roberto Clemente ($n = 1$, with the highest
initial success rate), has 80% posterior intervals of $(85,110)$ with
complete pooling, $(113, 184)$ with no pooling, $(93, 147)$ with the
basic hierarchical model, and $(87, 132)$ in the hierarchical logit
model; in reality, he had 127 hits in his remaining at bats, within
all the 80% intervals other than that of the complete pooling model.

The posterior produced by the model for the number of hits for the
rest of the season is overdispersed compared to a simple binomial
model based on a point estimate.  For example, if we take the
partially pooled estimate of 0.32 for Roberto Clemente's ability, the
prediction for number of hits based on the point estimate would be
$\mathrm{Binomial}(K^{\mathrm{new}}_1, 0.32)$, which we know analytically has a
standard deviation of $\sqrt{n \, \theta_n \, (1 - \theta_n)} = 8.9$,
which is quite a bit lower than the posterior standard deviation of 21
in the hierarchical model for $z_1$.  

Translating the posterior number of hits into a season batting
average, $(y_n + z_n) / (K_n + K^{\mathrm{new}}_n)$, we get an 80%
interval of

\[
\left( \frac{18 + 93}{45 + 367}, \frac{18 + 147}{45 + 367} \right) = (0.269, 0.400).
\]

The broad range shown here is an illustration of how poor binary data
on the order of a few dozen observations is for estimating chances of
success.


## Estimating Event Probabilities

The 80% interval in the partial pooling model coincidentally shows us
that our model estimates a roughly 10% chance of Roberto Clemente
batting 0.400 or better for the season based on batting 0.400 in his
first 45 at bats.  Not great, but non-trivial.  Rather than fishing
for the right quantile and hoping to get lucky, we can write a model
to direclty estimate event probabilities, such as Robert Clemente's
batting average is 0.400 or better for the season.

Event probabilities are defined as expectations of indicator functions
over parameters and data. For example, the probability of player $n$'s
batting average being 0.400 or better conditioned on the data $y$ is
defined by the conditional event probability

\[
\mathrm{Pr}\left[
\frac{(y_n + z_n)}{(45 + K^{\mathrm{new}}_n)} \geq 0.400 
\, \Big| \, 
y
\right]
\ = \
\int_{(0,1)^N}
 \mathrm{I}\left[\frac{(y_n + z_n)}{(45 + K^{\mathrm{new}}_n)} \geq 0.400\right]
       \ p(z_n \, | \, \theta_n, K^{\mathrm{new}}_n)
       \ p(\theta \, | \, y, K)
       \ \mathrm{d}\theta.
\]

The indicator function $\mathrm{I}[c]$ evaluates to 1 if the condition
$c$ is true and 0 if it is false.  Because it is just another
expectation with respect to the posterior, we can calculate this event
probability using MCMC as

\[
\mathrm{Pr}\left[\frac{(y_n + z_n)}{(45 + K^{\mathrm{new}}_n)} \geq
0.400 \, \Big| \, y \right]
\ \approx \
\frac{1}{M} \sum_{m=1}^M \mathrm{I}\left[\frac{(y_n + z_n^{(m)})}{(45 + K^{\mathrm{new}}_n)} \geq 0.400\right].
\]

This event is about the season batting average being greather than
0.400.  What if we care about ability (chance of success), not batting
average (success rate) for the rest of the season?  Then we would ask
the question of whether $\mathrm{Pr}[\theta_n > 0.4]$.  This is
defined as a weighted average over the prior and computed via MCMC as
the previous case.

\[
\mathrm{Pr}\left[\theta_n \geq 0.400 \, | \, y \right]
\ = \
\int_{(0,1)^N}
 \mathrm{I}\left[\theta_n \geq 0.400\right]
       \ p(z_n \, | \, \theta_n, K^{\mathrm{new}}_n)
       \ p(\theta \, | \, y, K)
       \ \mathrm{d}\theta
\ \approx \
\frac{1}{M} \sum_{m=1}^M \mathrm{I}[\theta_n^{(m)} \geq 0.400].
\]

In Stan, we just declare and define the indicators directly in the
generated quantities block.

```
generated quantities {
  ...
  int<lower=0, upper=1> some_ability_gt_350;  // Pr[some theta > 0.35]
  int<lower=0, upper=1> avg_gt_400[N];        // Pr[season avg of n] >= 0.400
  int<lower=0, upper=1> ability_gt_400[N];    // Pr[chance-of-success of n] >= 0.400
  ...
  some_ability_gt_350 <- (max(theta) > 0.35);
  for (n in 1:N)
    avg_gt_400[n] <- (((y[n] + z[n]) / (0.0 + K[n] + K_new[n])) > 0.400);
  for (n in 1:N)
    ability_gt_400[n] <- (theta[n] > 0.400);
  ...
}
```

The indicator function is not explicitly specified because Stan's
boolean operator greater-than returns either 0 or 1.  The only trick
to this code is the <code>0.0 + ...</code> in the fraction
computation, the purpose of which is to cast the value to real to
prevent integer division from kicking in and rounding down.

As usual, the expectations appear as the posterior means in the
summary.  We can summarize for all four models again.

Only the event indicator variables are printed---the parameter
estimates will be the same as before.

```{r comment=NA}
pars_to_print <- c("some_ability_gt_350", 
    "avg_gt_400[1]","avg_gt_400[5]", "avg_gt_400[10]", 
    "ability_gt_400[1]", "ability_gt_400[5]", "ability_gt_400[10]");
print(fit_pool, pars=pars_to_print, probs=c());
print(fit_no_pool, pars=pars_to_print, probs=c());
print(fit_hier, pars=pars_to_print, probs=c());
print(fit_hier_logit, pars=pars_to_print, probs=c());
```

The standard deviation and quantiles are not useful here and the
quantiles are supressed through an empty <code>probs</code> argument
to <code>print()</code>; we do want to see a reasonable effective
sample size estimate and no evidence of non-convergence in the form
for $\hat{R}$ values much greater than 1.

It is clear from the results that the probabilty of batting 0.400 or
better for the season is a different question than asking if the
player's ability is 0.400 or better; for example, with respect to the
basic partial pooling model, there is roughly an estimated 10% chance
of Roberto Clemente ($n = 1$) batting 0.400 or better for the season,
but only an estimated 8% chance that he has ability greater than
0.400.

The NaN values in the R-hat column result when every posterior draw is
the same; there is no variance, and hence R-hat is not defined.


## Multiple Comparisons

We snuck in a "multiple comparison" event in the last section, namely
whether there was some player with an a chance of success for hits of
.350 or greater.

With traditional significance testing over multiple trials, it is
common to adjust for falsely rejecting the null hypothesis (a
so-called <a
href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">Type
I error</a>) by inflating the conventional (and arguably far too low)
5% target for reporting "significance."

For example, suppose we have our 18 players with ability parameters
$\theta_n$ and we have $N$ null hypotheses of the form $H_0^n:
\theta_n < 0.350$.  Now suppose we evaluate each of these 18
hypotheses independently at the conventional $p = 0.05$ significance
level, giving each a 5% chance of rejecting the null hypothesis in
error.  When we run all 18 hypothesis tests, the overall chance of
falsely rejecting at least one of the null hypotheses is a whopping $1
- (1 - 0.05)^{18} = 0.60$.

The traditional solution to this problem is to apply a <a
href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni
adjustment</a> to control the false rejection rate; the typical
adjustment is to divide the $p$-value by the number of hypothesis
tests in the "family" (that is, the collective test being done).  Here
that sets the rate to $p = 0.05/18$, or approximately $p = 0.003$, and
results in a slightly less than 5% chance of falsely rejecting a null
hypothesis in error.

Although the Bonferroni correction does reduce the overall chance of
falsely rejecting a null hypothesis, it also reduces the statistical
power of the test to the same degree.  This means that many null
hypotheses will fail to be rejected in error.

Rather than doing classical multiple comparison adjustments to adjust
for false-discovery rate, such as a Bonferroni correction, Gelman et
al. (2012) suggest using a hierarchical model to perform partial
pooling instead.  As already shown, hierarchical models partially pool
the data, which pulls estimates toward the population mean with a
strength determined by the amount of observed variation in the
population (see also Figure 2 of (Gelman et al. 2012)).  This
automatically reduces the false-discovery rate, though not in a way
that is intrinsically calibrated to false discovery, which is good,
because reducing the overall false discovery rate in and of itself
reduces the true discovery rate at the same time.

The generated quantity <code>some_ability_gt_350</code> will be set to
1 if the maximum ability estimate in $\theta$ is greater than 0.35.
And thus the posterior mean of this generated quantity will be the
event probability

\[
\mathrm{Pr}[\mathrm{max}(\theta) > 0.350]
\ = \ \int_{(0,1)^N} \mathrm{I}[\mathrm{max}(\theta) > 0.35] \ p(\theta \, | \, y, K) \ \mathrm{d}\theta
\ \approx \ \frac{1}{M} \ \sum_{m=1}^M \ \mathrm{I}[\mathrm{max}(\theta^{(m)}) > 0.35]
\]

where $\theta^{(m)}$ is the sequence of posterior draws for the
ability parameter vector.  Stan reports this value as the posterior
mean of the generated quantity <code>some_ability_gt_350</code>, which
takes on the value $\mathrm{I}[\mathrm{max}(\theta^{(m)}) > 0.35]$ in
each iteration.

The probability estimate of there being a player with an ability
(chance of success) greater than 0.350 is roughly 0% in the complete
pooling model, 25% in the hierarchical logit model, 62% in the basic
hierarchical model, and 100% in the no pooling model.  Neither of the
partially pooled estimates would be considered significant at
conventional p=0.05 thresholds.  One way to get a handle on what's
going on is to inspect the posterior 80% intervals for
chance-of-success estimates in the first graph above.


## Ranking

In addition to multiple comparisons, we can use the simultaneous
estimation of the ability parameters to rank the items.  In this
section, we rank ballplayers by (estimated) hitting ability.

To implement ranking in Stan, we define a generated quantity
<code>rnk</code>to hold the ranks; that is, \code{rnk[2]} is
the rank from $1$ to $N$ of the second player.  First, we define a
local variable \code{dsc}, and sort it into descending order.

```
generated quantities {
  ...
  int<lower=1, upper=N> rnk[N];   // rnk[n] is rank of player n
  ...
  {
    int dsc[N];
    dsc <- sort_indices_desc(theta);
    for (n in 1:N)
      rnk[dsc[n]] <- n;
  }
  ...
}
```

After the call to <code>sort_indices_desc</code>, <code>dsc[n]</code>
holds the index of the $n$-th best item.  For example, if <code>rnk[2]
== 5</code>, so that player with identifier 2 is ranked 5th, then
<code>dsc[5] == 2</code>, because the fiftth rank player is the player
with identifier 2.  The final loop just puts the ranks into their
proper place in the <code>rnk</code> array.

In this example, the nested brackets produce a scope in which the
integer array variable <code>dsc</code> can be declared as a local
variable to hold the sorted indices.  Unlike variables declared at the
top of the generated quantities block, local variables are not saved
every iteration.

Of course, ranking players by ability makes no sense for the complete
pooling model, where every player is assumed to have the same ability.

We can print just the ranks and the 80% central interval for the basic pooling model.

```{r comment=NA}
print(fit_hier, "rnk", probs=c(0.1, 0.5, 0.9));
```

It is again abundantly clear from the posterior intervals that our
uncertainty is very great after only 45 at bats.

In the original Volume I BUGS example (see [OpenBUGS: Surgical
example](http://www.openbugs.net/Examples/Surgical.html)) of surgical
mortality, the posterior distribution over ranks was plotted for each
hospital.  It is now straightforward to reproduce that figure here for
the baseball data.

```{r comment=NA}
library(ggplot2);
df_rank <- data.frame(list(name = rep(as.character(df[[1,2]]), M),
                           rank = ss_hier$rnk[, 1]));
for (n in 2:N) {
  df_rank <- rbind(df_rank,
                   data.frame(list(name = rep(as.character(df[[n,2]]), M),
                              rank = ss_hier$rnk[, n])));
}
rank_plot <-
  ggplot(df_rank, aes(rank)) +
  stat_count(width=0.8) +
  facet_wrap(~ name) +
  scale_x_discrete(limits=c(1, 5, 10, 15)) +
  scale_y_discrete(name="posterior probability", breaks=c(0, 0.1 * M, 0.2 * M),
                   labels=c("0.0", "0.1", "0.2")) + 
  ggtitle("Rankings for Partial Pooling Model");
rank_plot;
```

We have followed the original BUGS presentation where the normalized
posterior frequency (i.e., the frequency divided by total count) is
reported as a probability on the y axis.  This plot will look very
different for the four different models (see exercises).

#### Who has the Highest Chance of Success?

We can use our ranking statistic to calculate the event probability
for item $n$ that the item has the highest chance of success using
MCMC as

\[
\mathrm{Pr}[\theta_n = \max(\theta)]
\ = \
\int_{\Theta} \mathrm{I}[\theta_n = \mathrm{max}(\theta)]
              \ p(\theta \, | \, y, K)
              \ \mathrm{d}\theta
\ \approx \ 
\frac{1}{M} \ \sum_{n=1}^N \mathrm{I}[\theta^{(m)}_n = \mathrm{max}(\theta^{(m)})].
\]

Like our other models, the partial pooling mitigates the implicit
multiple comparisons being done to calculate the probabilities of
rankings.  Contrast this with an approach that does a pairwise
significance test and then applies a false-discovery correction.

We can compute this straightforwardly using the rank data we have
already computed or we could compute it directly as above.  Because
$\mathrm{Pr}[\theta_n = \theta_{n'}] = 0$ for $n \neq n'$, we don't
have to worry about ties.

We define a new generated quantity for the value of the indicator and
define it using the already-computed ranks.

```
generated quantities {
  ...
  int<lower=0, upper=1> is_best[N];  // Pr[player n highest chance of success]
  ...
  for (n in 1:N)
    is_best[n] <- (rnk[n] == 1);
  ...
}
```

This means that <code>is_best[n]</code> will be 1 if player
<code>n</code> is the best player (in that iteration $m$).

We can then plot the results for the four models.

```{r comment=NA}
df_is_best_for <- function(name, ss) {
  is_best <- rep(NA, N);
  for (n in 1:N) {
    is_best[n] <- mean(ss$is_best[,n]);
  }
  return(data.frame(list(item=1:N, is_best=is_best, model=name)));
}

df_is_best <-  rbind(df_is_best_for("no pool", ss_no_pool),
                     df_is_best_for("hier", ss_hier),
                     df_is_best_for("hier (log odds)", ss_hier_logit));

is_best_plot <-
  ggplot(df_is_best, aes(x=item, y=is_best)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ model) +
  scale_y_continuous(name = "Pr[player is best]") +
  scale_x_discrete(name="player", breaks=c(1, 5, 10, 15)) +
  ggtitle("Who is the Best Player?");
is_best_plot;
```

The hierarchical plot is simply the first column of each of the
entries in the previous plot in the order they appear; the other plots
repeat the process for the no pooling and log-odds hierarchical model.

This question doesn't even make sense in the complete pooling model,
because the chance of success parameters are all the same by
definition.  In the remaining three models, the amount of pooling
directly determines the probabilities of being the best player.  For
example, Roberto Clemente, the top performing player in the first 45
at bats, has an estimated 35%, 25% and 18% chance of being the best
player according to the no-pooling, partial pooling, and partial
pooling of log odds models; the tenth best performing player, Ron
Swaboda, has an estimated 1%, 2% and 3% chance according to the same
models.  That is, the probability of being best goes down for high
performing players with more pooling, whereas it goes up for
below-average players.


## Posterior p-Values

A test statistic $T$ is a function from data to a real value.
Folllowing (Gelman et al. 2013), we will concentrate on four specific
test statistics for repeated binary trial data (though the choices we
make are general): minimum value, maximum value, sample mean, and
sample standard deviation.

Given a test statistic $T$ and data $y$, the Bayesian $p$-value is

\[
p_B = \mathrm{Pr}[T(y^{\mathrm{rep}}) \geq T(y) \, | \, y].
\]

Expanding this as an expectation of an indicator function, 

\[
p_B 
= \int_{\Theta} 
    \mathrm{I}[T(y^{\mathrm{rep}}) \geq T(y)] 
    \ p(\theta \, | \, y)
    \ \mathrm{d}\theta,
\]

leads to the obvious MCMC calculation based on $M$ draws of replicated
data $y^{\mathrm{rep}}$,

\[
p_B
\approx 
\frac{1}{M}
\
\sum_{m=1}^M
\mathrm{I}[T(y^{\mathrm{rep} \ (m)}) \geq T(y)].
\]


The test statistics are first defined for the observed data.  Because
they are functions purely of variables defined in the data block, they
can be defined in the transformed data block so that they will be
computed only once when the data is read in.

```
transformed data {
  real min_y;   // minimum successes
  real max_y;   // maximum successes
  real mean_y;  // sample mean successes
  real sd_y;    // sample std dev successes

  min_y <- min(y);
  max_y <- max(y);
  mean_y <- mean(to_vector(y));
  sd_y <- sd(to_vector(y));
}
```

The code to generate the replicated data is in the generated
quantities block.

```
generated quantities {
  ...
  int<lower=0> y_rep[N];      // replications for existing items
  ...
  for (n in 1:N)
    y_rep[n] <- binomial_rng(K[n], theta[n]);
  ...
}
```

Then the test statistics are defined in the generated quantities
block.

```
generated quantities {
  ...
  real<lower=0> min_y_rep;   // posterior predictive min replicated successes
  real<lower=0> max_y_rep;   // posterior predictive max replicated successes
  real<lower=0> mean_y_rep;  // posterior predictive sample mean replicated successes
  real<lower=0> sd_y_rep;    // posterior predictive sample std dev replicated successes
  ...
  min_y_rep <- min(y_rep);
  max_y_rep <- max(y_rep);
  mean_y_rep <- mean(to_vector(y_rep));
  sd_y_rep <- sd(to_vector(y_rep));
  ...
}
```

Finally, the actual $p$-value indicators are computed and assigned to
integer values.  The calls to <code>to_vector()</code> convert an
array of integers to an array of real values so that they are
appropriately typed to be the input to the standard deviation
calculation.

```
generated quantities {
  ...
  int<lower=0, upper=1> p_min;  // posterior predictive p-values
  int<lower=0, upper=1> p_max;
  int<lower=0, upper=1> p_mean;
  int<lower=0, upper=1> p_sd;
  ...
  p_min <- (min_y_rep >= min_y);
  p_max <- (max_y_rep >= max_y);
  p_mean <- (mean_y_rep >= mean_y);
  p_sd <- (sd_y_rep >= sd_y);
  ...
}
```

The fit from the Stan program can then be used to display the Bayesian
$p$-values for each of the models.

```{r comment=NA}
print(fit_pool, c("p_min", "p_max", "p_mean", "p_sd"), probs=c());
print(fit_no_pool, c("p_min", "p_max", "p_mean", "p_sd"), probs=c());
print(fit_hier, c("p_min", "p_max", "p_mean", "p_sd"), probs=c());
print(fit_hier_logit, c("p_min", "p_max", "p_mean", "p_sd"), probs=c());
```

The only worrisomely extreme value is the $p$-value for standard
deviation in the no-pooling model, where 99% of the simulated data
sets under the model had standard deviations among the number of hits
greater than the actual data.  Note that we could've constructed our
posterior p-values to run in the opposite direction, or could
formulate two-sided tests.


We can easily reproduce Figure 6.12 from (Gelman et al. 2013), which
shows the posterior predictive distribution for the test statistic,
the observed value as a vertical line, and the p-value for each of the
tests.  Here is the plot for the basic hierarchical model.

```{r comment=NA}
y_min <- min(y);
y_max <- max(y);
y_mean <- mean(y);
y_sd <- sd(y);

pvals_frame <- function(ss, model_name) {
  df_pvals_min <- data.frame(list(test_stat = rep("min", M), 
                                  replication = ss$min_y_rep),
                                  model = rep(model_name, M));
  df_pvals_max <- data.frame(list(test_stat = rep("max", M), 
                                  replication = ss$max_y_rep),
                                  model = rep(model_name, M));
  df_pvals_mean <- data.frame(list(test_stat = rep("mean", M), 
                                   replication = ss$mean_y_rep),	
				   model = rep(model_name, M));
  df_pvals_sd <- data.frame(list(test_stat = rep("sd", M), 
                                 replication = ss$sd_y_rep),
                                 model = rep(model_name, M));
  return(rbind(df_pvals_min, df_pvals_max, df_pvals_mean, df_pvals_sd));
}

df_pvals <- rbind(pvals_frame(ss_hier, "partial pool"),
                  pvals_frame(ss_hier_logit, "partial (logit)"),
                  pvals_frame(ss_pool, "complete pool"),
                  pvals_frame(ss_no_pool, "no pool"));

post_test_stat_plot <-
  ggplot(df_pvals, aes(replication)) +
  facet_grid(model ~ test_stat) +
  geom_histogram(binwidth = 0.5, colour="black", size = 0.25, fill="white") + 
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab("value in replicated data set") +
  geom_vline(aes(xintercept = y_val), 
             data = data.frame(y_val = rep(c(rep(y_min, M), rep(y_max, M), 
                                             rep(y_mean, M), rep(y_sd, M)), 4),
                               test_stat = df_pvals$test_stat,
                               replication = df_pvals$replication),
             colour = "blue", size = 0.25) +
  ggtitle("Posterior p-values")
post_test_stat_plot;
```


## Graphical Posterior Predictive Checks

In this section, we will compare the simulated data from the two
different hierarchical models, one with the Beta prior on the
chance-of-success scale and one with a normal prior on the log-odds
scale.  Following the advice of Gelman et al. (2013), we will take the
fitted parameters of the data set and generate replicated data sets,
then compare the replicated data sets visually to the actual data set.

As discussed by Gelman et al. (2013, p. 143), there is a choice of
which parameters to fix and which to simulate.  We can generate new
trials using fitted chance-of-success parameters ($\alpha$), or we can
generate new items using the population parameters ($\mu$ and
$\sigma$).  We'll provide both in the model below for comparison.  In
both cases, we simulate from posterior draws, not from the posterior
mean or other point estimates; Stan makes this easy with its generated
quantities block.  Then we'll plot several of those versus the real
data.

The Stan code to generate replicated data for existing items
(<code>y_rep</code>) and for new items drawn from the population
distribution (<code>y_pop_rep</code>) is in the generated quantities
block.

```
generated quantities {  
  ...
  int<lower=0> y_rep[N];      // replications for existing items
  int<lower=0> y_pop_rep[N];  // replications for simulated items
  ...
  for (n in 1:N)
    y_rep[n] <- binomial_rng(K[n], theta[n]);
  for (n in 1:N)
    y_pop_rep[n] <- binomial_rng(K[n], inv_logit(normal_rng(mu, sigma)));
}
```

The random population member generation is put through the
inverse-logit (logistic sigmoid) function to generate a probability,
then a replication generated based on the number of trials.  The
reason we have to go through looping is that Stan hasn't (yet)
vectorized the binomial RNGs; the reason we have to use the explicit
inverse-logit formulation is that there is no binomial-logit RNG.

For the basic hierarchical model, the population replication is
carried out as

```
  for (n in 1:N)
    y_pop_rep[n] <- binomial_rng(K[n], 
                                 beta_rng(phi * kappa,
                                          (1 - phi) * kappa));

```

In this case, the Beta variate is being generated inside the binomial
pseudorandom number generator.

We can now print the replicated values <code>y_rep</code> in both the
hierarchical and hierarchical logit models.

```{r comment=NA}
print(fit_hier, c("y_rep", "y_pop_rep[1]"), probs=c(0.1, 0.5, 0.9));
print(fit_hier_logit, c("y_rep", "y_pop_rep[1]"), probs=c(0.1, 0.5, 0.9));
```

The variable <code>y_rep</code> is ordered because it's based on the
actual items, which were sorted by success count in the original data.
On the other hand, the <code>y_pop_rep</code> values are fully
exchangeable, so indistinguishable in their posterior (other than MCMC
error).  Because the population replicates are exchangeable, we
only printed the first for each model.

We can plot some of the simulated data sets along with the original
data set to do a visual inspection as suggested by Gelman et
al. (2013).  Conveniently, RStan's <code>extract()</code> function has
already permuted the posterior draws, so we can just take the initial
fifteen for display.

```{r comment=NA}
df_post <- data.frame(list(dataset = rep("REAL", N),
                           y = y));
for (n in 1:15) {
  df_post <- rbind(df_post,
                   data.frame(list(dataset = rep(paste("repl ", n), N),
                                   y = ss_hier_logit$y_rep[n,])));
}
post_plot <-
  ggplot(df_post, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("Existing Item Replication (Normal Prior on Log Odds)");
post_plot;
```

And now we can do the same thing for the population-level replication; because the code's the same, we do not echo it to the output.

```{r comment=NA, echo=FALSE}
df_pop_post <- data.frame(list(dataset = rep("REAL", N),
                               y = y));
for (n in 1:15) {
  df_pop_post <- rbind(df_pop_post,
                       data.frame(list(dataset = rep(paste("repl ", n), N),
                                        y = ss_hier_logit$y_pop_rep[n,])));
}
post_pop_plot <-
  ggplot(df_pop_post, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("New Item Replication (Normal Prior on Log Odds)");
post_pop_plot;
```

The posterior simulations are not unreasonable for a binomial
likelihood, but are noticeably more spread out than the actual data.
This may actually have more to do with how the data were selected out
of all the major league baseball players than the actual data
distribution.  Efron and Morris (1975, p 312) write, "This sample was
chosen because we wanted between 30 and 50 at bats to assure a
satisfactory approximation of the binomial by the normal distribution
while leaving the bulk of at bats to be estimated.  We also wanted to
include an unusually good hitter (Clemente) to test the method with at
least one extreme parameter, a situation expected to be less favorable
to Stein's estimator.  Stein's estimator requires equal variances, or
in this situation, equal at bats, so the remaining 17 players are all
whom either the April 26 or May 3 *New York Times* reported with 45 at
bats."

Here are the same plots for the basic hierarchical model.  This time we suppress printing the ggplot code, which is the same as before, and just show the graphs.

```{r comment=NA, echo=FALSE}
df_post_beta <- data.frame(list(dataset = rep("REAL", N),
                                y = y));
for (n in 1:15) {
  df_post_beta <- rbind(df_post_beta,
                        data.frame(list(dataset = rep(paste("repl ", n), N),
                                        y = ss_hier$y_rep[n,])));
}
post_plot_beta <-
  ggplot(df_post_beta, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("Existing Item Replication (Beta Prior on Probability)");
post_plot_beta;
```

```{r comment=NA, echo=FALSE}
df_pop_post_beta <- data.frame(list(dataset = rep("REAL", N),
                               y = y));
for (n in 1:15) {
  df_pop_post_beta <- rbind(df_pop_post_beta,
                            data.frame(list(dataset = rep(paste("repl ", n), N),
                                             y = ss_hier$y_pop_rep[n,])));
}
post_pop_plot_beta <-
  ggplot(df_pop_post_beta, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("New Item Replication (Beta Prior on Probability)");
post_pop_plot_beta;
```

The plots for replications of existing items is radically different
with the beta prior, with the replications being much closer to the
originally observed values.  The replications for new items look
similar.  By eye, it looks like the beta prior is controlling the
posterior variance of individual item chance-of-success estimates much
better, despite producing wider posterior 80% intervals for chance of
success.


## Discussion

A hierarchical model introduces an estimation bias toward the
population mean and the stronger the bias, the less variance there is
in the estimates for the items.  Exactly how much bias and variance is
warranted can be estimated by further calibrating the model and
testing where its predictions do not bear out.  In this case, both
models yield similar estimates, with the logistic model trading a bit
more bias for variance in its estimates.  It is important to keep in
mind that both models are consistent in the sense that as the number
of trials increases, both models will converge to the empirical chance
of success for the ability parameters.


## Exercises

1. Calculate the Jacobian of the log-odds transform on a chance of
success and show that it produces the unit logistic
(i.e., $\mathsf{Logistic}(0,1)$) distribution.

1.  Generate fake data according to the pooling, no-pooling, and one
of the hierarchical models.  Fit the model and consider the coverage
of the posterior 80% intervals.

1.  By calculating the appropriate Jacobian, show that if $\kappa \sim
\mathsf{Pareto}(1, 1.5)$ (i.e., $p(\kappa) \propto
\kappa^{-\frac{5}{2}}$), then $\kappa^{-\frac{1}{2}}$ has a uniform
distribution. 

1.  Fit one of the data sets with a different number of initial trials
for each item.  What effect does the number of initial trials have on
the posterior?  Is there a way to quantify the effect?

1.  How sensitive is the basic no-pooling model to the choice of
prior?  Consider using knowledge of baseball (as in the logit example)
to provide a weakly informative prior for $\theta_n$.  How, if at all,
does this affect posterior inference?

1. Running on the larger baseball data set, how do the posterior means
and 80% intervals compare to those in the smaller data set for
$\kappa$ and $\phi$?  Is the result inevitable for a larger data set?

1. How sensitive is the hierarchical model to the priors on the
hyperparameters $\kappa, \phi$?  Consider a weakly informative prior
on $\phi$ and alternative distributional families for $\kappa$ (e.g.,
exponential).

1. Write a Stan model to predict $z_n$ based on the no pooling and
complete pooling models and compare those to the predictions based on
the hierarchical model.  Which is better calibrated in the sense of
having roughly the right number of actual values fall within the
prediction intervals?  Then, compare the prediction made from a
maximum likelihood point estimate of performance using a binomial
predictive likelihood.

1.  Why do the plots normalize properly (sum to 1) in both the
Rankings and Who's the Best Player plots?

1.  Lunn et al. (2013) contains a running analysis of pediatric
cardiac surgery mortality for 12 hospitals.  Reanalyze their data with
Stan using the models in this note.  The models will have to be
generalized to allow $K$ to vary by item $n$. The data is available
from Stan's example model repository as [BUGS Vol 1 examples:
Surgical](https://github.com/stan-dev/example-models/tree/master/bugs_examples/vol1/surgical)
and the WinBUGS project hosts the [original example
description](http://www.openbugs.net/Examples/Surgical.html) (which
has different counts from the data analyzed by Lunn et al.); Lunn et
al. analyze slightly different data in the BUGS book than in their
on-line example.

1.  How sensitive is the logistic model to the hyperprior?  If we made
it much less informative or much more informative, how much do the
fitted values differ?  How do the predictions differ?  Evaluate the
prior used by Lunn et al. (2013), which is
$\mathsf{Uniform}(-100,100)$ for $\mu$ and $\mathsf{Uniform}(0, 100)$
for $\sigma$.  Be sure to add constraints on the declarations of $\mu$
and $\sigma$ to match the constraints.  What happens if the width of
the uniform grows much wider or much narrower?

1. For the no pooling and complete pooling models (fixed priors),
derive the analytic posterior and code it using a random-number
generator in the generated quantities block.  

1. For the partial pooling models (hierarchical priors), marginalize
out the chance-of-success parameters ($\theta$ or $\alpha$) and sample
the higher-level parameters for the prior directly.

1.  Generate rankings plot for the other three models; compare and
contrast to the basic partial pooling model.

1.  Figure out how to take all the hard-coded output analysis in here
and make it dynamic by making better use of knitr.  And take all the
cut-and-paste duplicated code and wrap it into functions.


## References

* Betancourt, M. and Girolami, M. (2015) Hamiltonian Monte Carlo for
  hierarchical models. <i>Current Trends in Bayesian Methodology with
  Applications</i> <b>79</b>.

* Efron, B. and Morris, C. (1975) Data analysis using Stein's
  estimator and its generalizations. <i>Journal of the American
  Statistical Association</i> <b>70</b>(350), 311--319. [
  [pdf](http://www.medicine.mcgill.ca/epidemiology/hanley/bios602/MultilevelData/EfronMorrisJASA1975.pdf)
  ]

* Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A.,
  and Rubin, D. B. (2013) <i>Bayesian Data Analysis</i> 3rd
  Edition. Chapman & Hall/CRC Press, London.

* Gelman, A. and Hill, J. (2007) <i>Data Analysis Using Regression and
  Multilevel-Hierarchical Models</i>. Cambridge University Press,
  Cambridge, United Kingdom.

* Gelman, A., Hill, J., and Yajima, M. (2012) Why we (usually) don't
  have to worry about multiple comparisons. <i>Journal of Research on
  Educational Effectiveness</i> <b>5</b>, 189--211. [
  [pdf](http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf)
  ]

* Lunn, D., Jackson, C., Best, N., Thomas, A., and Spiegelhalter,
  D. (2013) <i>The BUGS Book: A Practical Introduction to Bayesian
  Analysis</i>.  Chapman & Hall/CRC Press.

* Spiegelhalter, D., Thomas, A., Best, N., & Gilks, W. (1996) BUGS 0.5
  Examples. MRC Biostatistics Unit, Institute of Public health,
  Cambridge, UK.

* Tarone, R. E. (1982) The use of historical control information in
  testing for a trend in proportions. <i>Biometrics</i>
  <b>38</b>(1):215--220.

## Data Sets Included

#### Rat tumors (N = 71)

Tarone (1982) provides a data set of tumor incidence in historical
control groups of rats; specifically endometrial stromal polyps in
female lab rats of type F344.  The data set is taken from the book
site for (Gelman et al. 2013):

* Data: <tt>rat-tumors.csv</tt>
* R script: <tt>rat-tumors.R</tt>
* Data source: [http://www.stat.columbia.edu/~gelman/book/data/rats.asc](http://www.stat.columbia.edu/~gelman/book/data/rats.asc)


#### Surgical mortality (N = 12)

Spiegelhalter et al. (1996) provide a data set of mortality rates
in 12 hospitals performing cardiac surgery in babies.  We just
manually entered the data from the paper; it is also available in
the Stan example models repository in R format.

* Data: [<tt>surgical-mortality.csv</tt>](surgical-mortality.csv)
* R script: <tt>surgical-mortality.R</tt>
* Data source: Unknown


#### Baseball hits 1970 (N = 18)

Efron and Morris (1975) provide a non-random sample of 18 players from
the 1970 Major League Baseball season.  A web site with the data is
available:

* Data: <tt>baseball-hits-1970.csv</tt>
* R script: <tt>baseball-hits-1970.R</tt>
* Data source: [http://www.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt](http://www.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt)


#### Baseball hits 1996 AL (N = 308)

Carpenter (2009) updates Efron and Morris's (1975) data set for the
entire set of players for the entire 1996 American League season of
Major League Baseball.  The data was originally downloaded from the
[baseball1.com](http://www.baseball1.com/).

* Data: <tt>baseball-hits-1996.csv</tt>
* R Script: <tt>baseball-hits-1996.R</tt>
* Data Source: [http://lingpipe-blog.com/2009/09/23/bayesian-estimators-for-the-beta-binomial-model-of-batting-ability](http://lingpipe-blog.com/2009/09/23/bayesian-estimators-for-the-beta-binomial-model-of-batting-ability/)


## Appendix: Full Stan Programs

####  <code>pool.stan</code>

```{r comment=NA, echo=FALSE, comment=NA}
file_path <- "pool.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```

#### <code>no-pool.stan</code>

```{r comment=NA, echo=FALSE}
file_path <- "no-pool.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```


#### <code>hier.stan</code>

```{r comment=NA, echo=FALSE}
file_path <- "hier.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```


#### <code>hier-logit.stan</code>

```{r comment=NA, echo=FALSE}
file_path <- "hier-logit.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```


