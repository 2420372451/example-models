





 


## Multiple Comparisons

With traditional significance testing over multiple trials, it is common to adjust for falsely rejecting the null hypothesis (a so-called <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">Type I error</a>) by inflating the conventional (and arguably far too low) 5% target for reporting "significance."  

For example, suppose we have our 18 players with ability parameters $\theta_n$ and we have $N$ null hypotheses of the form $H_0^n: \theta_n < 0.350$.  Now suppose we evaluate each of these 18 hypotheses independently at the conventional $p = 0.05$ significance level, giving each a 5% chance of rejecting the null hypothesis in error.  When we run all 18 hypothesis tests, the overall chance of falsely rejecting at least one of the null hypotheses is a whopping $1 - (1 - 0.05)^{18} = 0.60$.
The traditional solution to this problem is to apply a <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni adjustment</a> to control the false rejection rate;  the typical adjustment is to divide the $p$-value by the number of hypothesis tests in the "family" (that is, the collective test being done).  Here that sets the rate to $p = 0.05/18$, or approximately $p = 0.003$, and results in a slightly less than 5% chance of falsely rejecting a null hypothesis in error.  

Although the Bonferroni correction does reduce the overall chance of falsely rejecting a null hypothesis, it also reduces the statistical power of the test to the same degree.  This means that many null hypotheses will fail to be rejected in error. 

Rather than doing classical multiple comparison adjustments to adjust for false-discovery rate, such as a Bonferroni correction, Gelman et al. (2012) suggest using a hierarchical model to perform partial pooling instead.  As illustrated above, hierarchical models partially pool the data, which pulls estimates toward the population mean with a strength determined by the amount of observed variation in the population (see also Figure 2 of (Gelman et al. 2012)).  This automatically reduces the false-discovery rate, though not in a way that is intrinsically calibrated to false discovery, which is good, because reducing the overall false discovery rate in and of itself reduces the true discovery rate at the same time.

We code the model in Stan using the generated quantities block in file <code>hier-compare.stan</code>.
```{r comment=NA, echo=FALSE}
file_path <- "hier-compare.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```
The generated quantity <code>some_ability_gt_350</code> will be set to 1 if the maximum ability estimate in $\theta$ is greater than 0.35.  And thus the posterior mean of this generated quantity will be the event probability
\[
\mathrm{Pr}[\mathrm{max}(\theta) > 0.350]
\ = \ \int_{(0,1)^N} \mathrm{I}[\mathrm{max}(\theta) > 0.35] \ p(\theta \, | \, y, K) \ \mathrm{d}\theta
\ \approx \ \frac{1}{M} \ \sum_{m=1}^M \ \mathrm{I}[\mathrm{max}(\theta^{(m)}) > 0.35]
\]
where $\theta^{(m)}$ is the sequence of posterior draws for the ability parameter vector.  Stan reports this value as the posterior mean of the generated quantity <code>some_ability_gt_350</code>, which takes on the value $\mathrm{I}[\mathrm{max}(\theta^{(m)}) > 0.35]$ in each iteration. 
Expectations are computed via MCMC as usual.
```{r  comment=NA, results='hide'}
fit_compare <- stan("hier-compare.stan", data=c("N", "K", "y", "J"), 
                    control=list(stepsize=0.01, adapt_delta=0.99));
ss_compare <- extract(fit_compare);
```

Only the event indicator variables are printed---the parameter estimates will be the same as before.
```{r comment=NA}
print(fit_compare, pars=c("some_ability_gt_350"), probs=c());
```
The probability estimate of there being a player with an ability (chance of success) greater than 0.350 is around 60%.  This number would not indicate "significance" at the conventional level, but does provide an estimate of how likely the result is that is directly interpretable as a probability.  Considering the early figure comparing complete pooling, no pooling, and partial pooling, the complete pooling estimate would estimate a close to zero probability that some player had an ability higher than 0.350, as it would mean all players had an ability that high.  In the no pooling model, the probability is quite high that at least one of the players has an ability of 0.350 or greater;  the model estimates that Roberto Clemente alone is more than 60% likely to have a 0.350 or greater ability and provides 10 players or so for whom 0.350 is in their 80% central posterior interval, giving them a 10% chance or higher of having a 0.350 ability.  In contrast, only four of the players in the partial pooling example have 80% intervals that cross the 0.350 line.

The multiple comparison "adjustment" is being done implicitly through the hierarchical model, which pulls each estimate toward the population average.  There are no adjusted pairwise comparisons, just an overall comparsion.

### Ranking

In addition to multiple comparisons, we can use the simultaneous estimation of the ability parameters to rank the items;  here we are ranking ballplayers by hitting ability.

We code the model in Stan using the generated quantities block in <code>rank.stan</code>:
```{r comment=NA, echo=FALSE}
file_path <- "rank.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```
It is fit, the values are extracted.
```{r  comment=NA, results='hide'}
fit_rank <- stan("rank.stan", data=c("N", "K", "y"),
                 control=list(stepsize=0.01, adapt_delta=0.99));
ss_rank <- extract(fit_rank);
```
We can print just the ranks and the 80% central interval.
```{r comment=NA}
print(fit_rank, "rnk", probs=c(0.1, 0.5, 0.9));
```
It is clear from the posterior intervals that our uncertainty is very great after only 45 at bats.

In the original Volume I BUGS example (see [OpenBUGS: Surgical example](http://www.openbugs.net/Examples/Surgical.html)) of surgical mortality, the ranks of each hospital was plotted.  We can reproduce that figure here for the baseball data.

```{r comment=NA}
library(ggplot2);
df_rank <- data.frame(list(name = rep(as.character(df[[1,2]]), 4000),
                           rank = ss_rank$rnk[, 1]));
for (n in 2:N) {
  df_rank <- rbind(df_rank,
                   data.frame(list(name = rep(as.character(df[[n,2]]), 4000),
                              rank = ss_rank$rnk[, n])));
}
rank_plot <-
  ggplot(df_rank, aes(rank)) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(1, 5, 10, 15)) +
  scale_y_discrete(name="posterior probability", breaks=c(0, 400, 800),
                   labels=c("0.0", "0.1", "0.2")) + 
  facet_wrap(~ name);
rank_plot;
```

We have followed the original BUGS presentation where the normalized posterior frequency (i.e., the frequency divided by total count) is reported as a probability on the y axis.




### Posterior Prediction and Replication

It is straightforward to both evaluate the posterior predictive density for new data $y^{\mathrm{new}}$ and to generate replicated data sets $y^{\mathrm{rep}}$ for posterior predictive checks.

#### Posterior Predictive Distribution




#### Evaluating Posterior Predictions

Because the posterior predictive density is defined as an expectation, it can be computed (on the log scale) for new observations $y^{\mathrm{new}}$ via MCMC using the posterior draws $\theta^{(m)}$ as
\[
\log p(y^{\mathrm{new}} \, | \, y)
\ \approx \
\frac{1}{M} \ \sum_{m=1}^M \log p(y^{\mathrm{new}} \, | \, \theta^{(m)}).
\]
All that is needed is a way to evaluate the likelihood $p(\tilde{y} \, | \, \theta)$ for fixed $\tilde{y}$ and $\theta$.

We now have two models whose posterior predictions can be compared.

#### Posterior Predictions: Beta Priors on Chance-of-Success

First we need to extract the new observations---the remaing at bats and hits for each player.
```{r comment=NA}
K_new <- df$RemainingAt.Bats;
y_new <- df$RemainingHits;
```

Then we can use them in a Stan model to evaluate the posterior predictive density of the new data, which is in file <code>hier-heldout.stan</code>. 
```{r comment=NA, echo=FALSE}
file_path <- "hier-heldout.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```

The model is fit as usual.
```{r  comment=NA, results='hide'}
fit_hier_heldout <- stan("hier-heldout.stan", data=c("N", "K", "y", "K_new", "y_new"),
                       control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier_heldout <- extract(fit_hier_heldout);
```

Then we print the predictive density, the mean of which is our MCMC calculation of the posterior predictive density.
```{r comment=NA}
print(fit_hier_heldout, "log_p_new", probs=c(0.1, 0.5, 0.9));
```
The posterior mean, roughly -120, is the value of $p(y^{\mathrm{new}} \, | \, y)$ --- the predictive density of the new data conditioned on the old (and the model, of course).  Although the mean is the posterior log density, the posterior interval is quite wide, with some parameter values providing much higher density, as is to be expected in a hierarchical model.

Now here is the same exercise for the model with a normal prior on log-odds, starting with the model in <code>hier-logit-heldout.stan</code>,
```{r comment=NA, echo=FALSE}
file_path <- "hier-logit-heldout.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```

```{r  comment=NA, results='hide'}
fit_hier_logit_heldout <- stan("hier-logit-heldout.stan",
                               data=c("N", "K", "y", "K_new", "y_new"),
                               control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier_logit_heldout <- extract(fit_hier_logit_heldout);
```

```{r comment=NA}
print(fit_hier_logit_heldout, "log_p_new", probs=c(0.1, 0.5, 0.9));
```
Here the posterior log density is only -100 or so, which given that it's 18 players on the log scale, is a fair bit better than the -120 for the model with Beta priors on the chances of success.

#### Simulating Replicated Data Sets

To simulate replicated data sets from the posterior predictive distribution, we can use forward simulation based on the MCMC draws $\theta^{(m)}$.  For each posterior draw $\theta^{(m)}$, a replicated data set can be produced by forward simulation by drawing
${y^{\mathrm{rep}}}^{(m)}$ according to the likelihood $p(y \, | \, \theta^{(m)})$.  This produces a Markov chain of replicated values which can be analyzed as usual to compute expectations of quantities of interest.


#### Model Evaluation and Comparison

We now have two competing models giving us slightly different answers.  So the next thing to do is some posterior predictive checks to see if the models adequately capture the distributione of the data.  Following the advice of Gelman et al. (2013), we will take the fitted parameters of the data set and generate fake data sets. 

As discussed by Gelman et al. (2013, p. 143), there is a choice of which parameters to fix and which to simulate.  We can generate new trials using fitted chance-of-success parameters ($\alpha$), or we can generate new items using the population parameters ($\mu$ and $\sigma$).  We'll provide both in the model below for comparison.  In both cases, we simulate from posterior draws, not from the posterior mean or other point estimates; Stan makes this easy with its generated quantities block.  Then we'll plot several of those versus the real data.

The Stan file is <code>hier-logit-post.stan</code>, which is based on the more efficient non-centered parameterization:
```{r comment=NA, echo=FALSE}
file_path <- "hier-logit-post.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```
The generated quantities block uses random number generators either to generate a new trials or to first generate a new chance of success and then new trials.

The model is fit as usual, only now with the non-centered parameterization, we can get away with default parameters.
```{r comment=NA, results='hide'}
fit_hier_logit_post <- stan("hier-logit-post.stan", data=c("N", "K", "y"),
                            control=list(stepsize=0.01, adapt_delta=0.95));
ss_hier_logit_post <- extract(fit_hier_logit_post);
```
We can now print the replicated values <code>y_rep</code> as usual.
```{r comment=NA}
print(fit_hier_logit_post, c("y_rep", "y_pop_rep"), probs=c(0.1, 0.5, 0.9));
```
We printed both for comparison.  The variable <code>y_rep</code> is ordered because it's based on the actual items, which were sorted by success count in the original data.  On the other hand, the <code>y_pop_rep</code> values are fully exchangeable, so indistinguishable in their posterior (other than MCMC error).

We can plot some of the simulated data sets along with the original data set to do a visual inspection as suggested by Gelman et al. (2013);  the <code>extract()</code> function has already permuted the posterior draws, so we can just take the initial fifteen for display.
```{r comment=NA}
df_post <- data.frame(list(dataset = rep("REAL", N),
                           y = y));
for (n in 1:15) {
  df_post <- rbind(df_post,
                   data.frame(list(dataset = rep(paste("repl ", n), N),
                                   y = ss_hier_logit_post$y_rep[n,])));
}
post_plot <-
  ggplot(df_post, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("Existing Item Replication (Normal Prior on Log Odds)");
post_plot;
```

And now we can do the same thing for the population-level replication; because the code's the same, we do not echo it to the output.
```{r comment=NA, echo=FALSE}
df_pop_post <- data.frame(list(dataset = rep("REAL", N),
                               y = y));
for (n in 1:15) {
  df_pop_post <- rbind(df_pop_post,
                       data.frame(list(dataset = rep(paste("repl ", n), N),
                                        y = ss_hier_logit_post$y_pop_rep[n,])));
}
post_pop_plot <-
  ggplot(df_pop_post, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("New Item Replication (Normal Prior on Log Odds)");
post_pop_plot;
```

The posterior simulations are not unreasonable for a binomial likelihood, but are noticeably more spread out than the actual data.  This may actually have more to do with how the data were selected out of all the major league baseball players than the actual data distribution.  Efron and Morris (1975, p 312) write, "This sample was chosen because we wanted between 30 and 50 at bats to assure a satisfactory approximation of the binomial by the normal distribution while leaving the bulk of at bats to be estimated.  We also wanted to include an unusually good hitter (Clemente) to test the method with at least one extreme parameter, a situation expected to be less favorable to Stein's estimator.  Stein's estimator requires equal variances, or in this situation, equal at bats, so the remaining 17 players are all whom either the April 26 or May 3 *New York Times* reported with 45 at bats."

Now to see how posterior replications look for the model with the hierarchical Beta prior.

```{r comment=NA, echo=FALSE}
file_path <- "hier-post.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```
Now the generated quantities uses the Beta prior for the population replications.

We fit and print values as before.
```{r comment=NA, results='hide'}
fit_hier_post <- stan("hier-post.stan", data=c("N", "K", "y"),
                      control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier_post <- extract(fit_hier_post);
```
We can now print the replicated values <code>y_rep</code> as usual.
```{r comment=NA}
print(fit_hier_post, c("y_rep", "y_pop_rep"), probs=c(0.1, 0.5, 0.9));
```

And we can print out the results as before.  This time we suppress printing the ggplot code, which is the same as before, and just show the graphs.

```{r comment=NA, echo=FALSE}
df_post_beta <- data.frame(list(dataset = rep("REAL", N),
                                y = y));
for (n in 1:15) {
  df_post_beta <- rbind(df_post_beta,
                        data.frame(list(dataset = rep(paste("repl ", n), N),
                                        y = ss_hier_post$y_rep[n,])));
}
post_plot_beta <-
  ggplot(df_post_beta, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("Existing Item Replication (Beta Prior on Probability)");
post_plot_beta;
```

```{r comment=NA, echo=FALSE}
df_pop_post_beta <- data.frame(list(dataset = rep("REAL", N),
                               y = y));
for (n in 1:15) {
  df_pop_post_beta <- rbind(df_pop_post_beta,
                            data.frame(list(dataset = rep(paste("repl ", n), N),
                                             y = ss_hier_post$y_pop_rep[n,])));
}
post_pop_plot_beta <-
  ggplot(df_pop_post_beta, aes(y)) +
  facet_wrap(~ dataset) +
  stat_count(width=0.8) +
  scale_x_discrete(limits=c(5, 10, 15, 20)) +
  scale_y_discrete(name="count", limits=c(0, 2, 4)) +
  ggtitle("New Item Replication (Beta Prior on Probability)");
post_pop_plot_beta;
```

The plots for replications of existing items is radicall different with the beta prior, with the replications being much closer to the originally observed values.   The replications for new items look similar.  By eye, it looks like the beta prior is controlling the posterior variance of individual item chance-of-success estimates much better, despite producing wider posterior 80% intervals for chance of success.

### Posterior p-Values

A test statistic $T$ is a function from data to a real value.  We will concentrate on four specific test statistics for repeated binary trial data (though the choices we make are general): minimum value, maximum value, sample mean, and sample standard deviation.

Given a test statistic $T$ and data $y$, the Bayesian $p$-value is
\[
p_B = \mathrm{Pr}[T(y^{\mathrm{rep}}) \geq T(y) \, | \, y].
\]
Expanding this as an expectation of an indicator function, 
\[
p_B 
= \int_{\Theta} 
    \mathrm{I}[T(y^{\mathrm{rep}}) \geq T(y)] 
    \ p(\theta \, | \, y)
    \ mathrm{d}\theta,
\]
leads to the obvious MCMC calculation based on $M$ draws of $y^{\mathrm{rep}}$,
\[
p_B
\approx 
\frac{1}{M}
\
\sum_{m=1}^M
\mathrm{I}[T(y^{\mathrm{rep} \ (m)}) \geq T(y)].
\]

The Stan program to calculate the posterior $p$-values for the minimum, maximum, sample mean, and sample standard deviation is in <code>hier-pvals.stan</code>.
```{r comment=NA, echo=FALSE}
file_path <- "hier-pvals.stan";
lines <- readLines(file_path, encoding="ASCII");
for (n in 1:length(lines)) cat(lines[n],'\n');
```

We fit as usual.
```{r comment=NA, results='hide'}
fit_hier_pvals <- stan("hier-pvals.stan", data=c("N", "K", "y"),
                      control=list(stepsize=0.01, adapt_delta=0.99));
ss_hier_pvals <- extract(fit_hier_pvals);
```

Then print the calculated Bayesian posterior $p$-values.
```{r comment=NA}
print(fit_hier_pvals, c("p_min", "p_max", "p_mean", "p_sd"),
      probs=c(0.1, 0.5, 0.9));
```

It is now easy to reproduce Figure 6.12 from (Gelman et al. 2013), which shows the posterior predictive distribution for the test statistic, the observed value as a vertical line, and the p-value for each of the tests.
```{r comment=NA}
y_min <- min(y);
y_max <- max(y);
y_mean <- mean(y);
y_sd <- sd(y);
df_pvals_min <- data.frame(list(test_stat = rep("min", 4000), 
                                replication = ss_hier_pvals$min_y_rep));
df_pvals_max <- data.frame(list(test_stat = rep("max", 4000), 
                                replication = ss_hier_pvals$max_y_rep));
df_pvals_mean <- data.frame(list(test_stat = rep("mean", 4000), 
                                 replication = ss_hier_pvals$mean_y_rep));
df_pvals_sd <- data.frame(list(test_stat = rep("sd", 4000), 
                               replication = ss_hier_pvals$sd_y_rep));

df_pvals <- rbind(df_pvals_min, df_pvals_max, df_pvals_mean, df_pvals_sd);

post_test_stat_plot <-
  ggplot(df_pvals, aes(replication)) +
  facet_wrap(~ test_stat) +
  geom_histogram(binwidth = 1, colour="black", size = 0.25, fill="white") + 
  geom_vline(aes(xintercept = y_val), 
             data = data.frame(y_val = c(rep(y_min, 4000), rep(y_max, 4000), 
                                         rep(y_mean, 4000), rep(y_sd, 4000)),
                               test_stat = df_pvals$test_stat,
                               replication = df_pvals$replication),
             colour = "blue", size = 0.5) +
  ggtitle("Posterior p-Values (Beta Prior)");
post_test_stat_plot;
```
### Discussion

A hierarchical model introduces an estimation bias toward the population mean and the stronger the bias, the less variance there is in the estimates for the items.  Exactly how much bias and variance is warranted can be estimated by further calibrating the model and testing where its predictions do not bear out.  In this case, both models yield similar estimates, with the logistic model trading a bit more bias for variance in its estimates.  It is important to keep in mind that both models are consistent in the sense that as the number of trials increases, both models will converge to the empirical chance of success for the ability parameters.

### Exercises

1.  Generate fake data according to the one of the models.  Fit the model and consider the coverage of the posterior 80% intervals.

1.  Extend one of the Stan programs to allow a different number of initial trials for each item.  Simulating data for this situation, what effect does the number of initial trials have on the posterior?

1. How sensitive is the hierarchical model to the priors on the hyperparameters $\kappa, \phi$?  Consider a weakly informative prior on $\phi$ and alternative distributional families for $\kappa$ (e.g., exponential).  

1. Write a Stan model to predict $z_n$ based on the no pooling and complete pooling models and compare those to the predictions based on the hierarchical model.  Which is better calibrated in the sense of having roughly the right number of actual values fall within the prediction intervals?  Then, compare the prediction made from a maximum likelihood point estimate of performance using a binomial predictive likelihood.

1.  Given the hierarchical model, estimate the probability for each player that they have the highest batting ability.  What is the probability that Roberto Clemente will lead these 18 players (given the modeling assumptions)?  Compare that to the probability that each player will end the season with the highest batting average (observed success rate for season).  Write a Stan model to compute both values and fit it.

1.  Lunn et al. (2013) contains a running analysis of pediatric cardiac surgery mortality for 12 hospitals.  Reanalyze their data with Stan using the models in this note.  The models will have to be generalized to allow $K$ to vary by item $n$. The data is available from Stan's example model repository as [BUGS Vol 1 examples: Surgical](https://github.com/stan-dev/example-models/tree/master/bugs_examples/vol1/surgical) and the WinBUGS project hosts the [original example description](http://www.openbugs.net/Examples/Surgical.html) (which has different counts from the data analyzed by Lunn et al.); Lunn et al. analyze slightly different data in the BUGS book than in their on-line example.

1.  How sensitive is the logistic model to the hyperprior?  If we made it much less informative or much more informative, how much do the fitted values differ?  How do the predictions differ?  Evaluate the prior used by Lunn et al. (2013), which is $\mathsf{Uniform}(-100,100)$ for $\mu$ and $\mathsf{Uniform}(0, 100)$ for $\sigma$.  Be sure to add constraints on the declarations of $\mu$ and $\sigma$ to match the constraints.  What happens if the width of the uniform grows much wider or much narrower?

1.  Figure out how to take all the hard-coded output analysis in here and make it dynamic by making better use of knitr.  And take all the cut-and-paste duplicated code and wrap it into functions.

### References

* Betancourt, M. and Girolami, M. (2015) Hamiltonian Monte Carlo for hierarchical models. <i>Current Trends in Bayesian Methodology with Applications</i> <b>79</b>.

* Efron, B. and Morris, C. (1975) Data analysis using Stein's estimator and its generalizations. <i>Journal of the American Statistical Association</i> <b>70</b>(350), 311--319. [ [pdf](http://www.medicine.mcgill.ca/epidemiology/hanley/bios602/MultilevelData/EfronMorrisJASA1975.pdf) ]

* Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013) <i>Bayesian Data Analysis</i> 3rd Edition. Chapman & Hall/CRC Press, London.

* Gelman, A. and Hill, J. (2007) <i>Data Analysis Using Regression and Multilevel-Hierarchical Models</i>. Cambridge University Press, Cambridge, United Kingdom.

* Gelman, A., Hill, J., and Yajima, M. (2012) Why we (usually) don't have to worry about multiple comparisons. <i>Journal of Research on Educational Effectiveness</i> <b>5</b>, 189--211. [ [pdf](http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf) ]

* Lunn, D., Jackson, C., Best, N., Thomas, A., and Spiegelhalter, D. (2013) <i>The BUGS Book: A Practical Introduction to Bayesian Analysis</i>.  Chapman & Hall/CRC Press.