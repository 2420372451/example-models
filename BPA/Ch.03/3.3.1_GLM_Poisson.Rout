
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R は、自由なソフトウェアであり、「完全に無保証」です。 
一定の条件に従えば、自由にこれを再配布することができます。 
配布条件の詳細に関しては、'license()' あるいは 'licence()' と入力してください。 

R は多くの貢献者による共同プロジェクトです。 
詳しくは 'contributors()' と入力してください。 
また、R や R のパッケージを出版物で引用する際の形式については 
'citation()' と入力してください。 

'demo()' と入力すればデモをみることができます。 
'help()' とすればオンラインヘルプが出ます。 
'help.start()' で HTML ブラウザによるヘルプがみられます。 
'q()' と入力すれば R を終了します。 

 [以前にセーブされたワークスペースを復帰します] 

> ## 3. Introduction to the generalized linear model (GLM): The simplest
> ## model for count data
> ## 3.3. Poisson GLM in R and WinBUGS for modeling times series of
> ## counts
> ## 3.3.1. Generation and analysis of simulated data
> 
> library(rstan)
 要求されたパッケージ ggplot2 をロード中です 
rstan (Version 2.8.2, packaged: 2015-11-26 15:27:02 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> set.seed(123)
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLM_Poisson.data.R")
> 
> ## Initial values
> inits <- function() list(alpha = runif(1, -2, 2),
+                          beta1 = runif(1, -3, 3))
> 
> ## Parameters monitored
> params <- c("alpha", "beta1", "beta2", "beta3", "lambda")
> 
> ## MCMC settings
> ni <- 2000
> nt <- 1
> nb <- 1000
> nc <- 4
> 
> ## Call Stan from R
> out <- stan("GLM_Poisson.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, thin = nt, iter = ni, warmup = nb,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=6874 on localhost:11377 at 20:05:47.944
starting worker pid=6882 on localhost:11377 at 20:05:48.116
starting worker pid=6890 on localhost:11377 at 20:05:48.269
starting worker pid=6898 on localhost:11377 at 20:05:48.430

SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.11208 seconds (Warm-up)
#                0.10726 seconds (Sampling)
#                0.21934 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.105697 seconds (Warm-up)
#                0.122571 seconds (Sampling)
#                0.228268 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.110012 seconds (Warm-up)
#                0.115907 seconds (Sampling)
#                0.225919 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.106543 seconds (Warm-up)
#                0.117906 seconds (Sampling)
#                0.224449 seconds (Total)

> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLM_Poisson.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

               mean se_mean   sd     2.5%      25%      50%      75%    97.5%
alpha          4.28    0.00 0.03     4.23     4.26     4.28     4.30     4.34
beta1          1.25    0.00 0.04     1.16     1.22     1.25     1.28     1.33
beta2          0.07    0.00 0.02     0.02     0.05     0.07     0.09     0.11
beta3         -0.23    0.00 0.02    -0.28    -0.25    -0.23    -0.21    -0.18
lambda[1]     32.20    0.08 3.13    26.31    30.08    32.11    34.22    38.55
lambda[2]     30.02    0.06 2.48    25.27    28.39    29.99    31.62    35.03
lambda[3]     28.48    0.05 2.00    24.60    27.18    28.46    29.78    32.54
lambda[4]     27.47    0.03 1.66    24.23    26.36    27.42    28.54    30.78
lambda[5]     26.91    0.03 1.43    24.20    25.95    26.89    27.85    29.69
lambda[6]     26.74    0.03 1.28    24.36    25.86    26.70    27.60    29.18
lambda[7]     26.94    0.02 1.20    24.71    26.09    26.92    27.73    29.28
lambda[8]     27.49    0.03 1.17    25.32    26.67    27.47    28.26    29.80
lambda[9]     28.38    0.03 1.19    26.13    27.57    28.36    29.17    30.75
lambda[10]    29.63    0.04 1.23    27.32    28.80    29.59    30.43    32.12
lambda[11]    31.25    0.04 1.30    28.80    30.36    31.20    32.09    33.85
lambda[12]    33.26    0.05 1.38    30.64    32.32    33.21    34.16    36.08
lambda[13]    35.70    0.05 1.47    32.90    34.72    35.65    36.66    38.69
lambda[14]    38.60    0.05 1.56    35.61    37.56    38.55    39.62    41.78
lambda[15]    42.02    0.06 1.66    38.87    40.91    41.98    43.12    45.37
lambda[16]    46.00    0.06 1.76    42.70    44.83    45.96    47.15    49.45
lambda[17]    50.61    0.07 1.86    47.14    49.37    50.57    51.82    54.24
lambda[18]    55.91    0.07 1.95    52.24    54.59    55.85    57.20    59.69
lambda[19]    61.95    0.07 2.04    58.14    60.56    61.89    63.31    65.98
lambda[20]    68.81    0.07 2.13    64.82    67.37    68.74    70.26    73.08
lambda[21]    76.55    0.07 2.23    72.38    75.05    76.45    78.07    80.98
lambda[22]    85.20    0.07 2.33    80.86    83.61    85.12    86.74    89.81
lambda[23]    94.81    0.07 2.45    90.15    93.12    94.72    96.44    99.71
lambda[24]   105.38    0.07 2.60   100.38   103.60   105.33   107.11   110.62
lambda[25]   116.90    0.07 2.78   111.57   114.99   116.86   118.73   122.40
lambda[26]   129.31    0.07 3.01   123.54   127.25   129.30   131.29   135.32
lambda[27]   142.50    0.07 3.28   136.21   140.30   142.53   144.71   149.00
lambda[28]   156.33    0.08 3.58   149.43   153.89   156.37   158.73   163.42
lambda[29]   170.56    0.09 3.89   163.05   167.87   170.54   173.17   178.26
lambda[30]   184.92    0.10 4.19   176.76   182.05   184.92   187.74   193.29
lambda[31]   199.05    0.11 4.45   190.55   196.04   199.08   202.05   207.89
lambda[32]   212.55    0.11 4.64   203.61   209.42   212.52   215.65   221.89
lambda[33]   224.96    0.11 4.74   215.91   221.73   224.94   228.18   234.54
lambda[34]   235.77    0.11 4.77   226.64   232.46   235.73   239.00   245.37
lambda[35]   244.50    0.10 4.82   235.37   241.18   244.41   247.78   254.04
lambda[36]   250.65    0.08 5.00   241.22   247.17   250.55   254.03   260.62
lambda[37]   253.82    0.09 5.51   243.23   250.08   253.79   257.46   264.71
lambda[38]   253.66    0.10 6.46   241.28   249.29   253.61   257.91   266.25
lambda[39]   249.99    0.14 7.87   234.88   244.64   249.83   255.25   265.64
lambda[40]   242.74    0.19 9.61   224.29   236.25   242.41   249.10   261.88
lp__       17473.34    0.04 1.41 17469.58 17472.67 17473.68 17474.36 17475.06
           n_eff Rhat
alpha        883 1.01
beta1        894 1.00
beta2        998 1.01
beta3        817 1.01
lambda[1]   1366 1.01
lambda[2]   1569 1.00
lambda[3]   1869 1.00
lambda[4]   2298 1.00
lambda[5]   2488 1.00
lambda[6]   2507 1.00
lambda[7]   2306 1.00
lambda[8]   1845 1.00
lambda[9]   1425 1.00
lambda[10]  1174 1.00
lambda[11]  1019 1.00
lambda[12]   922 1.00
lambda[13]   861 1.00
lambda[14]   822 1.00
lambda[15]   798 1.01
lambda[16]   785 1.01
lambda[17]   784 1.01
lambda[18]   793 1.01
lambda[19]   814 1.01
lambda[20]   852 1.01
lambda[21]   913 1.01
lambda[22]  1005 1.00
lambda[23]  1138 1.00
lambda[24]  1312 1.00
lambda[25]  1524 1.00
lambda[26]  1854 1.00
lambda[27]  1928 1.00
lambda[28]  1940 1.00
lambda[29]  1898 1.00
lambda[30]  1837 1.00
lambda[31]  1786 1.00
lambda[32]  1761 1.00
lambda[33]  1794 1.00
lambda[34]  1945 1.00
lambda[35]  2329 1.00
lambda[36]  4000 1.00
lambda[37]  4000 1.00
lambda[38]  4000 1.00
lambda[39]  3192 1.00
lambda[40]  2503 1.00
lp__        1213 1.01

Samples were drawn using NUTS(diag_e) at Wed Jan  6 20:05:51 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   ユーザ   システム       経過  
    19.443      1.610     25.388 
