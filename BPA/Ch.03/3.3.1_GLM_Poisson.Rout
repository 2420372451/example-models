
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ## 3. Introduction to the generalized linear model (GLM): The simplest
> ## model for count data
> ## 3.3. Poisson GLM in R and WinBUGS for modeling times series of
> ## counts
> ## 3.3.1. Generation and analysis of simulated data
> 
> library(rstan)
Loading required package: ggplot2
rstan (Version 2.8.1, packaged: 2015-11-18 17:18:35 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> set.seed(123)
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLM_Poisson.data.R")
> 
> ## Initial values
> inits <- function() list(alpha = runif(1, -2, 2),
+                          beta1 = runif(1, -3, 3))
> 
> ## Parameters monitored
> params <- c("alpha", "beta1", "beta2", "beta3", "lambda")
> 
> ## MCMC settings
> ni <- 2000
> nt <- 1
> nb <- 1000
> nc <- 4
> 
> ## Call Stan from R
> out <- stan("GLM_Poisson.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, thin = nt, iter = ni, warmup = nb,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=5111 on localhost:11743 at 19:17:17.732
starting worker pid=5119 on localhost:11743 at 19:17:17.856
starting worker pid=5127 on localhost:11743 at 19:17:17.997
starting worker pid=5135 on localhost:11743 at 19:17:18.138

SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.102521 seconds (Warm-up)
#                0.100042 seconds (Sampling)
#                0.202563 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.104794 seconds (Warm-up)
#                0.121838 seconds (Sampling)
#                0.226632 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.09948 seconds (Warm-up)
#                0.102363 seconds (Sampling)
#                0.201843 seconds (Total)


SAMPLING FOR MODEL 'GLM_Poisson' NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.101876 seconds (Warm-up)
#                0.088302 seconds (Sampling)
#                0.190178 seconds (Total)

> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLM_Poisson.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

               mean se_mean   sd     2.5%      25%      50%      75%    97.5%
alpha          4.28    0.00 0.03     4.22     4.26     4.29     4.30     4.34
beta1          1.25    0.00 0.04     1.16     1.22     1.25     1.28     1.33
beta2          0.07    0.00 0.02     0.02     0.05     0.07     0.09     0.11
beta3         -0.23    0.00 0.02    -0.28    -0.25    -0.23    -0.21    -0.19
lambda[1]     32.14    0.09 3.20    26.13    29.94    32.07    34.20    38.58
lambda[2]     29.97    0.07 2.55    25.11    28.20    29.93    31.63    35.13
lambda[3]     28.43    0.05 2.07    24.52    27.01    28.41    29.80    32.56
lambda[4]     27.43    0.04 1.72    24.20    26.25    27.43    28.57    30.78
lambda[5]     26.87    0.03 1.47    24.03    25.85    26.86    27.85    29.79
lambda[6]     26.70    0.03 1.32    24.18    25.80    26.67    27.59    29.26
lambda[7]     26.91    0.03 1.23    24.60    26.07    26.89    27.74    29.29
lambda[8]     27.46    0.03 1.19    25.24    26.64    27.46    28.27    29.79
lambda[9]     28.36    0.03 1.20    26.11    27.54    28.34    29.18    30.71
lambda[10]    29.61    0.03 1.24    27.27    28.76    29.62    30.45    32.03
lambda[11]    31.23    0.04 1.30    28.75    30.33    31.22    32.10    33.77
lambda[12]    33.24    0.04 1.37    30.58    32.30    33.23    34.15    35.97
lambda[13]    35.68    0.04 1.46    32.81    34.69    35.69    36.64    38.60
lambda[14]    38.59    0.05 1.56    35.49    37.52    38.61    39.60    41.71
lambda[15]    42.01    0.05 1.65    38.76    40.90    42.02    43.08    45.31
lambda[16]    46.00    0.06 1.75    42.51    44.83    46.02    47.13    49.54
lambda[17]    50.61    0.06 1.85    46.92    49.39    50.62    51.80    54.36
lambda[18]    55.92    0.06 1.95    52.03    54.63    55.91    57.17    59.88
lambda[19]    61.97    0.06 2.05    57.86    60.62    61.98    63.29    66.11
lambda[20]    68.84    0.07 2.15    64.52    67.43    68.86    70.23    73.18
lambda[21]    76.58    0.07 2.25    72.07    75.10    76.62    78.04    81.10
lambda[22]    85.24    0.07 2.36    80.48    83.72    85.27    86.81    89.98
lambda[23]    94.86    0.07 2.49    89.90    93.25    94.89    96.46    99.79
lambda[24]   105.44    0.07 2.64   100.16   103.71   105.49   107.15   110.63
lambda[25]   116.97    0.07 2.82   111.45   115.09   117.02   118.82   122.51
lambda[26]   129.38    0.07 3.05   123.41   127.35   129.42   131.36   135.32
lambda[27]   142.59    0.08 3.31   136.07   140.35   142.61   144.74   149.03
lambda[28]   156.42    0.09 3.60   149.46   153.98   156.36   158.76   163.55
lambda[29]   170.65    0.09 3.90   163.00   168.02   170.58   173.28   178.36
lambda[30]   185.01    0.10 4.19   176.76   182.16   184.94   187.83   193.24
lambda[31]   199.14    0.11 4.44   190.47   196.11   199.06   202.15   207.87
lambda[32]   212.63    0.11 4.62   203.55   209.50   212.52   215.73   221.86
lambda[33]   225.01    0.11 4.73   215.65   221.86   224.91   228.11   234.55
lambda[34]   235.81    0.10 4.78   226.25   232.57   235.72   238.92   245.31
lambda[35]   244.50    0.10 4.84   234.91   241.23   244.39   247.66   253.99
lambda[36]   250.62    0.09 5.05   240.71   247.22   250.55   253.99   260.67
lambda[37]   253.74    0.09 5.57   242.62   249.99   253.71   257.45   264.71
lambda[38]   253.53    0.11 6.53   240.70   249.09   253.54   257.85   266.49
lambda[39]   249.80    0.15 7.90   234.86   244.25   249.70   255.07   265.51
lambda[40]   242.50    0.23 9.60   224.44   235.69   242.33   248.98   261.55
lp__       17473.30    0.04 1.42 17469.71 17472.59 17473.64 17474.32 17475.08
           n_eff Rhat
alpha       1083    1
beta1        982    1
beta2       1071    1
beta3        804    1
lambda[1]   1257    1
lambda[2]   1379    1
lambda[3]   1541    1
lambda[4]   1741    1
lambda[5]   2024    1
lambda[6]   2117    1
lambda[7]   2020    1
lambda[8]   1794    1
lambda[9]   1557    1
lambda[10]  1349    1
lambda[11]  1222    1
lambda[12]  1136    1
lambda[13]  1075    1
lambda[14]  1034    1
lambda[15]  1009    1
lambda[16]   996    1
lambda[17]   994    1
lambda[18]  1003    1
lambda[19]  1026    1
lambda[20]  1062    1
lambda[21]  1114    1
lambda[22]  1189    1
lambda[23]  1288    1
lambda[24]  1409    1
lambda[25]  1540    1
lambda[26]  1660    1
lambda[27]  1748    1
lambda[28]  1786    1
lambda[29]  1784    1
lambda[30]  1764    1
lambda[31]  1751    1
lambda[32]  1771    1
lambda[33]  1857    1
lambda[34]  2072    1
lambda[35]  2533    1
lambda[36]  3347    1
lambda[37]  4000    1
lambda[38]  3860    1
lambda[39]  2752    1
lambda[40]  1819    1
lp__        1039    1

Samples were drawn using NUTS(diag_e) at Wed Dec 16 19:17:21 2015.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   user  system elapsed 
  1.642   0.186   5.098 
