
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R は、自由なソフトウェアであり、「完全に無保証」です。 
一定の条件に従えば、自由にこれを再配布することができます。 
配布条件の詳細に関しては、'license()' あるいは 'licence()' と入力してください。 

R は多くの貢献者による共同プロジェクトです。 
詳しくは 'contributors()' と入力してください。 
また、R や R のパッケージを出版物で引用する際の形式については 
'citation()' と入力してください。 

'demo()' と入力すればデモをみることができます。 
'help()' とすればオンラインヘルプが出ます。 
'help.start()' で HTML ブラウザによるヘルプがみられます。 
'q()' と入力すれば R を終了します。 

 [以前にセーブされたワークスペースを復帰します] 

> ## 3. Introduction to the generalized linear model (GLM): The simplest
> ## model for count data
> ## 3.5. Binomial GLM for modeling bounded counts or proportions
> ## 3.5.1. Generation and analysis of simulated data
> 
> library(rstan)
 要求されたパッケージ ggplot2 をロード中です 
rstan (Version 2.8.2, packaged: 2015-11-26 15:27:02 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> set.seed(123)
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLM_Binomial.data.R")
> 
> ## Initial values
> inits <- function() list(alpha = runif(1, -1, 1),
+                          beta1 = runif(1, -1, 1),
+                          beta2 = runif(1, -1, 1))
> 
> ## Parameters monitored
> params <- c("alpha", "beta1", "beta2", "p")
> 
> ## MCMC settings
> ni <- 2000
> nt <- 1
> nb <- 1000
> nc <- 4
> 
> ## Call Stan from R
> out <- stan("GLM_Binomial.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, thin = nt, iter = ni, warmup = nb,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=6992 on localhost:11463 at 20:06:11.534
starting worker pid=7000 on localhost:11463 at 20:06:11.701
starting worker pid=7008 on localhost:11463 at 20:06:11.860
starting worker pid=7016 on localhost:11463 at 20:06:12.022

SAMPLING FOR MODEL 'GLM_Binomial' NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.055432 seconds (Warm-up)
#                0.062876 seconds (Sampling)
#                0.118308 seconds (Total)


SAMPLING FOR MODEL 'GLM_Binomial' NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.059468 seconds (Warm-up)
#                0.062867 seconds (Sampling)
#                0.122335 seconds (Total)


SAMPLING FOR MODEL 'GLM_Binomial' NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.065504 seconds (Warm-up)
#                0.066562 seconds (Sampling)
#                0.132066 seconds (Total)


SAMPLING FOR MODEL 'GLM_Binomial' NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4, Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4, Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4, Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4, Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4, Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4, Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4, Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
#  Elapsed Time: 0.059779 seconds (Warm-up)
#                0.065746 seconds (Sampling)
#                0.125525 seconds (Total)

> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLM_Binomial.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff
alpha     1.02    0.00 0.06     0.89     0.97     1.02     1.06     1.14  1432
beta1    -0.17    0.00 0.07    -0.31    -0.22    -0.17    -0.12    -0.03  1688
beta2    -0.92    0.00 0.15    -1.21    -1.02    -0.92    -0.82    -0.63  1252
p[1]      0.59    0.00 0.03     0.53     0.56     0.59     0.61     0.64  1867
p[2]      0.60    0.00 0.03     0.55     0.59     0.60     0.62     0.65  1925
p[3]      0.62    0.00 0.02     0.58     0.61     0.62     0.64     0.66  1992
p[4]      0.64    0.00 0.02     0.60     0.62     0.64     0.65     0.68  2067
p[5]      0.65    0.00 0.02     0.62     0.64     0.65     0.66     0.69  2143
p[6]      0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.69  2208
p[7]      0.68    0.00 0.01     0.65     0.67     0.68     0.69     0.70  2244
p[8]      0.69    0.00 0.01     0.66     0.68     0.69     0.70     0.71  2232
p[9]      0.70    0.00 0.01     0.67     0.69     0.70     0.70     0.72  2169
p[10]     0.70    0.00 0.01     0.68     0.70     0.70     0.71     0.73  2067
p[11]     0.71    0.00 0.01     0.69     0.70     0.71     0.72     0.73  1948
p[12]     0.72    0.00 0.01     0.70     0.71     0.72     0.73     0.74  1816
p[13]     0.72    0.00 0.01     0.70     0.72     0.72     0.73     0.75  1694
p[14]     0.73    0.00 0.01     0.71     0.72     0.73     0.74     0.75  1603
p[15]     0.73    0.00 0.01     0.71     0.72     0.73     0.74     0.75  1538
p[16]     0.73    0.00 0.01     0.71     0.73     0.73     0.74     0.76  1493
p[17]     0.73    0.00 0.01     0.71     0.73     0.74     0.74     0.76  1463
p[18]     0.74    0.00 0.01     0.71     0.73     0.74     0.74     0.76  1445
p[19]     0.74    0.00 0.01     0.71     0.73     0.74     0.74     0.76  1437
p[20]     0.73    0.00 0.01     0.71     0.73     0.73     0.74     0.76  1438
p[21]     0.73    0.00 0.01     0.71     0.72     0.73     0.74     0.76  1447
p[22]     0.73    0.00 0.01     0.70     0.72     0.73     0.74     0.75  1465
p[23]     0.72    0.00 0.01     0.70     0.72     0.73     0.73     0.75  1493
p[24]     0.72    0.00 0.01     0.69     0.71     0.72     0.73     0.74  1533
p[25]     0.71    0.00 0.01     0.69     0.71     0.71     0.72     0.74  1589
p[26]     0.71    0.00 0.01     0.68     0.70     0.71     0.72     0.73  1665
p[27]     0.70    0.00 0.01     0.67     0.69     0.70     0.71     0.72  1762
p[28]     0.69    0.00 0.01     0.66     0.68     0.69     0.70     0.71  1879
p[29]     0.68    0.00 0.01     0.65     0.67     0.68     0.69     0.70  2011
p[30]     0.67    0.00 0.01     0.64     0.66     0.67     0.68     0.69  2152
p[31]     0.66    0.00 0.01     0.63     0.65     0.66     0.66     0.68  2284
p[32]     0.64    0.00 0.01     0.61     0.63     0.64     0.65     0.67  2372
p[33]     0.63    0.00 0.01     0.60     0.62     0.63     0.64     0.65  2392
p[34]     0.61    0.00 0.02     0.58     0.60     0.61     0.62     0.64  2344
p[35]     0.59    0.00 0.02     0.55     0.58     0.59     0.60     0.63  2253
p[36]     0.57    0.00 0.02     0.53     0.56     0.57     0.58     0.61  2141
p[37]     0.55    0.00 0.02     0.51     0.54     0.55     0.57     0.60  2024
p[38]     0.53    0.00 0.03     0.48     0.51     0.53     0.55     0.58  1922
p[39]     0.50    0.00 0.03     0.45     0.49     0.50     0.52     0.56  1837
p[40]     0.48    0.00 0.03     0.42     0.46     0.48     0.50     0.54  1764
lp__  -1628.33    0.03 1.21 -1631.49 -1628.90 -1628.03 -1627.45 -1626.94  1275
      Rhat
alpha    1
beta1    1
beta2    1
p[1]     1
p[2]     1
p[3]     1
p[4]     1
p[5]     1
p[6]     1
p[7]     1
p[8]     1
p[9]     1
p[10]    1
p[11]    1
p[12]    1
p[13]    1
p[14]    1
p[15]    1
p[16]    1
p[17]    1
p[18]    1
p[19]    1
p[20]    1
p[21]    1
p[22]    1
p[23]    1
p[24]    1
p[25]    1
p[26]    1
p[27]    1
p[28]    1
p[29]    1
p[30]    1
p[31]    1
p[32]    1
p[33]    1
p[34]    1
p[35]    1
p[36]    1
p[37]    1
p[38]    1
p[39]    1
p[40]    1
lp__     1

Samples were drawn using NUTS(diag_e) at Wed Jan  6 20:06:15 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   ユーザ   システム       経過  
    18.903      0.654     23.091 
