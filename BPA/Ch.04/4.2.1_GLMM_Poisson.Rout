
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R は、自由なソフトウェアであり、「完全に無保証」です。 
一定の条件に従えば、自由にこれを再配布することができます。 
配布条件の詳細に関しては、'license()' あるいは 'licence()' と入力してください。 

R は多くの貢献者による共同プロジェクトです。 
詳しくは 'contributors()' と入力してください。 
また、R や R のパッケージを出版物で引用する際の形式については 
'citation()' と入力してください。 

'demo()' と入力すればデモをみることができます。 
'help()' とすればオンラインヘルプが出ます。 
'help.start()' で HTML ブラウザによるヘルプがみられます。 
'q()' と入力すれば R を終了します。 

> ## 4. Introduction to random effects: Conventional Poisson GLMM for
> ## count data
> ## 4.2. Accounting for overdispersion by random effects-modeling in R
> ## and WinBUGS
> ## 4.2.1. Generation and analysis of simulated data
> 
> library(rstan)
 要求されたパッケージ ggplot2 をロード中です 
rstan (Version 2.8.2, packaged: 2015-11-26 15:27:02 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLMM_Poisson.data.R")
> 
> ## Initial values
> inits <- function() list(alpha = runif(1, -2, 2),
+                          beta1 = runif(1, -3, 3),
+                          sd = runif(1, 0, 1))
> 
> ## Parameters monitored
> params <- c("alpha", "beta1", "beta2", "beta3", "lambda", "sigma",
+             "eps")
> 
> # MCMC settings
> ni <- 15000
> nt <- 10
> nb <- 5000
> nc <- 4
> 
> ## Call Stan from R
> out <- stan("GLMM_Poisson.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, iter = ni, warmup = nb, thin = nt,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=7138 on localhost:11491 at 20:08:41.997
starting worker pid=7146 on localhost:11491 at 20:08:42.158
starting worker pid=7154 on localhost:11491 at 20:08:42.312
starting worker pid=7162 on localhost:11491 at 20:08:42.470

SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 1).

Chain 1, Iteration:     1 / 15000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 2).

Chain 2, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 1, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 1, Iteration:  3000 / 15000 [ 20%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 3).

Chain 3, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 2, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 1, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 1, Iteration:  5001 / 15000 [ 33%]  (Sampling)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 4).

Chain 4, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 2, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 3, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 1, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 2, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 3, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 2, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 4, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 3, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 3, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 4, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 2, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 1, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 4, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 2, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 3, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 4, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 4, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 2, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 3, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 1, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 4, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 2, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 3, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 1, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 4, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 3, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 2, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 4, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 2, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 3, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 4, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 1, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 2, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.78425 seconds (Warm-up)
#                3.2923 seconds (Sampling)
#                5.07656 seconds (Total)


Chain 3, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 4, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 4, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.69615 seconds (Warm-up)
#                2.81964 seconds (Sampling)
#                4.51579 seconds (Total)


Chain 3, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.70085 seconds (Warm-up)
#                3.4054 seconds (Sampling)
#                5.10626 seconds (Total)


Chain 1, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 1, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.35695 seconds (Warm-up)
#                5.5457 seconds (Sampling)
#                6.90265 seconds (Total)

> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLMM_Poisson.
4 chains, each with iter=15000; warmup=5000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

               mean se_mean    sd     2.5%      25%      50%      75%    97.5%
alpha          4.30    0.00  0.03     4.24     4.28     4.30     4.32     4.36
beta1          1.25    0.00  0.05     1.15     1.21     1.25     1.28     1.34
beta2          0.07    0.00  0.02     0.02     0.06     0.07     0.09     0.12
beta3         -0.23    0.00  0.03    -0.28    -0.24    -0.23    -0.21    -0.17
lambda[1]     32.26    0.06  3.41    25.81    29.91    32.13    34.53    39.27
lambda[2]     30.59    0.05  2.82    25.33    28.64    30.48    32.39    36.43
lambda[3]     29.09    0.04  2.44    24.44    27.41    29.00    30.73    34.04
lambda[4]     28.24    0.04  2.14    24.34    26.77    28.14    29.60    32.80
lambda[5]     27.13    0.03  1.92    23.29    25.91    27.12    28.37    30.95
lambda[6]     27.37    0.03  1.84    24.03    26.16    27.30    28.48    31.18
lambda[7]     27.25    0.03  1.73    23.89    26.15    27.22    28.36    30.69
lambda[8]     27.77    0.03  1.67    24.52    26.71    27.74    28.84    31.12
lambda[9]     28.88    0.03  1.72    25.60    27.75    28.81    29.91    32.53
lambda[10]    30.04    0.03  1.78    26.60    28.89    29.99    31.13    33.69
lambda[11]    32.09    0.03  1.97    28.52    30.80    31.92    33.24    36.48
lambda[12]    34.49    0.04  2.14    30.73    33.10    34.29    35.68    39.18
lambda[13]    36.37    0.03  2.12    32.31    34.99    36.30    37.68    40.74
lambda[14]    39.13    0.04  2.31    34.67    37.70    39.06    40.56    43.87
lambda[15]    42.93    0.04  2.47    38.22    41.32    42.88    44.37    48.08
lambda[16]    47.68    0.05  2.76    42.84    45.81    47.45    49.23    53.95
lambda[17]    51.92    0.05  2.82    46.74    50.10    51.84    53.56    58.00
lambda[18]    56.25    0.05  2.98    50.08    54.38    56.36    58.20    61.84
lambda[19]    63.14    0.05  3.26    56.82    61.11    63.02    65.13    70.03
lambda[20]    71.72    0.07  3.76    65.27    69.20    71.31    73.76    80.43
lambda[21]    75.64    0.10  4.03    66.71    73.31    76.00    78.37    82.50
lambda[22]    86.86    0.07  4.00    78.53    84.34    86.82    89.26    95.37
lambda[23]    95.87    0.07  4.34    86.52    93.38    96.02    98.57   104.16
lambda[24]   106.38    0.09  4.82    96.03   103.58   106.50   109.43   115.48
lambda[25]   119.54    0.08  5.11   109.00   116.32   119.60   122.61   130.00
lambda[26]   132.11    0.09  5.69   120.47   128.61   132.13   135.44   143.55
lambda[27]   146.88    0.10  6.16   134.64   143.12   146.58   150.37   160.39
lambda[28]   157.74    0.13  6.83   143.14   153.69   158.16   162.15   170.74
lambda[29]   168.22    0.27  8.50   148.60   163.09   169.36   174.26   182.08
lambda[30]   197.17    0.24  9.51   182.37   190.39   195.73   202.40   219.11
lambda[31]   205.73    0.14  8.33   189.67   200.41   205.48   210.72   222.90
lambda[32]   213.48    0.21  9.09   193.49   207.98   214.15   219.65   229.49
lambda[33]   236.92    0.20  9.69   220.17   230.50   236.07   242.17   258.37
lambda[34]   247.76    0.16  9.56   230.65   241.43   246.99   253.22   269.15
lambda[35]   255.60    0.16  9.60   237.02   249.59   254.96   261.34   275.94
lambda[36]   261.88    0.17  9.83   243.48   255.77   261.47   267.51   283.87
lambda[37]   263.01    0.16  9.84   243.04   256.92   263.17   269.09   282.59
lambda[38]   263.86    0.16 10.11   243.40   257.49   263.82   270.12   284.65
lambda[39]   262.48    0.18 10.99   241.17   255.22   262.25   269.45   284.76
lambda[40]   252.10    0.19 11.65   229.59   244.20   252.19   259.85   275.43
sigma          0.04    0.00  0.02     0.01     0.02     0.04     0.05     0.09
eps[1]        -0.01    0.00  0.04    -0.12    -0.03    -0.01     0.01     0.07
eps[2]         0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.10
eps[3]         0.01    0.00  0.04    -0.08    -0.02     0.00     0.03     0.10
eps[4]         0.01    0.00  0.05    -0.07    -0.01     0.01     0.03     0.12
eps[5]        -0.01    0.00  0.04    -0.11    -0.03     0.00     0.01     0.07
eps[6]         0.01    0.00  0.04    -0.08    -0.02     0.00     0.03     0.10
eps[7]        -0.01    0.00  0.05    -0.11    -0.03     0.00     0.02     0.08
eps[8]        -0.01    0.00  0.04    -0.11    -0.03     0.00     0.01     0.07
eps[9]         0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.09
eps[10]       -0.01    0.00  0.04    -0.10    -0.02     0.00     0.02     0.08
eps[11]        0.01    0.00  0.04    -0.08    -0.01     0.00     0.03     0.11
eps[12]        0.02    0.00  0.05    -0.06    -0.01     0.01     0.04     0.13
eps[13]        0.00    0.00  0.04    -0.10    -0.02     0.00     0.02     0.09
eps[14]       -0.01    0.00  0.04    -0.11    -0.03     0.00     0.02     0.08
eps[15]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.09
eps[16]        0.02    0.00  0.04    -0.06    -0.01     0.01     0.04     0.12
eps[17]        0.01    0.00  0.04    -0.08    -0.01     0.00     0.03     0.10
eps[18]       -0.01    0.00  0.04    -0.11    -0.03    -0.01     0.01     0.06
eps[19]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.09
eps[20]        0.02    0.00  0.04    -0.05     0.00     0.01     0.04     0.13
eps[21]       -0.03    0.00  0.05    -0.15    -0.05    -0.02     0.00     0.04
eps[22]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.08
eps[23]       -0.01    0.00  0.04    -0.10    -0.03     0.00     0.01     0.07
eps[24]       -0.01    0.00  0.04    -0.10    -0.03    -0.01     0.01     0.06
eps[25]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.08
eps[26]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.08
eps[27]        0.01    0.00  0.04    -0.07    -0.01     0.00     0.03     0.09
eps[28]       -0.01    0.00  0.04    -0.11    -0.03    -0.01     0.01     0.06
eps[29]       -0.04    0.00  0.05    -0.15    -0.06    -0.03     0.00     0.03
eps[30]        0.04    0.00  0.05    -0.03     0.00     0.03     0.06     0.15
eps[31]        0.01    0.00  0.04    -0.07    -0.01     0.00     0.03     0.09
eps[32]       -0.02    0.00  0.04    -0.12    -0.05    -0.02     0.00     0.04
eps[33]        0.02    0.00  0.04    -0.04     0.00     0.01     0.04     0.11
eps[34]        0.02    0.00  0.04    -0.05     0.00     0.01     0.04     0.10
eps[35]        0.01    0.00  0.04    -0.06    -0.01     0.01     0.03     0.09
eps[36]        0.01    0.00  0.03    -0.06    -0.01     0.00     0.02     0.08
eps[37]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.01     0.06
eps[38]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.01     0.06
eps[39]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.07
eps[40]       -0.01    0.00  0.04    -0.10    -0.03    -0.01     0.01     0.06
lp__       18289.41    1.59 26.52 18252.18 18270.56 18283.73 18302.65 18358.63
           n_eff Rhat
alpha       3625 1.00
beta1       3000 1.00
beta2       3060 1.00
beta3       3026 1.00
lambda[1]   3182 1.00
lambda[2]   3107 1.00
lambda[3]   3365 1.00
lambda[4]   3440 1.00
lambda[5]   3783 1.00
lambda[6]   3334 1.00
lambda[7]   3594 1.00
lambda[8]   3309 1.00
lambda[9]   3867 1.00
lambda[10]  3820 1.00
lambda[11]  3622 1.00
lambda[12]  3103 1.00
lambda[13]  3915 1.00
lambda[14]  4000 1.00
lambda[15]  3733 1.00
lambda[16]  2894 1.00
lambda[17]  3419 1.00
lambda[18]  3673 1.00
lambda[19]  3919 1.00
lambda[20]  2903 1.00
lambda[21]  1524 1.00
lambda[22]  3669 1.00
lambda[23]  3771 1.00
lambda[24]  3171 1.00
lambda[25]  3866 1.00
lambda[26]  3899 1.00
lambda[27]  3798 1.00
lambda[28]  2801 1.00
lambda[29]  1015 1.00
lambda[30]  1584 1.00
lambda[31]  3675 1.00
lambda[32]  1879 1.00
lambda[33]  2457 1.00
lambda[34]  3363 1.00
lambda[35]  3623 1.00
lambda[36]  3487 1.00
lambda[37]  4000 1.00
lambda[38]  3902 1.00
lambda[39]  3791 1.00
lambda[40]  3737 1.00
sigma        517 1.01
eps[1]      2698 1.00
eps[2]      3823 1.00
eps[3]      3992 1.00
eps[4]      3613 1.00
eps[5]      3731 1.00
eps[6]      3945 1.00
eps[7]      3684 1.00
eps[8]      4000 1.00
eps[9]      4000 1.00
eps[10]     3912 1.00
eps[11]     3884 1.00
eps[12]     3248 1.00
eps[13]     4000 1.00
eps[14]     3382 1.00
eps[15]     4000 1.00
eps[16]     3046 1.00
eps[17]     3599 1.00
eps[18]     3645 1.00
eps[19]     4000 1.00
eps[20]     2557 1.00
eps[21]     1429 1.00
eps[22]     3683 1.00
eps[23]     4000 1.00
eps[24]     3635 1.00
eps[25]     4000 1.00
eps[26]     3888 1.00
eps[27]     3701 1.00
eps[28]     3682 1.00
eps[29]     1045 1.00
eps[30]     1272 1.00
eps[31]     3882 1.00
eps[32]     1958 1.00
eps[33]     2206 1.00
eps[34]     2874 1.00
eps[35]     3580 1.00
eps[36]     3559 1.00
eps[37]     3832 1.00
eps[38]     3561 1.00
eps[39]     3810 1.00
eps[40]     2685 1.00
lp__         279 1.01

Samples were drawn using NUTS(diag_e) at Wed Jan  6 20:08:50 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   ユーザ   システム       経過  
    20.042      0.669     29.446 
