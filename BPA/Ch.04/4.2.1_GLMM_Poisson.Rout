
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## 4. Introduction to random effects: Conventional Poisson GLMM for
> ## count data
> ## 4.2. Accounting for overdispersion by random effects-modeling in R
> ## and WinBUGS
> ## 4.2.1. Generation and analysis of simulated data
> 
> library(rstan)
Loading required package: ggplot2
rstan (Version 2.8.1, packaged: 2015-11-18 17:18:35 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLMM_Poisson.data.R")
> 
> ## Initial values
> inits <- function() list(alpha = runif(1, -2, 2),
+                          beta1 = runif(1, -3, 3),
+                          sd = runif(1, 0, 1))
> 
> ## Parameters monitored
> params <- c("alpha", "beta1", "beta2", "beta3", "lambda", "sigma",
+             "eps")
> 
> # MCMC settings
> ni <- 15000
> nt <- 10
> nb <- 5000
> nc <- 4
> 
> ## Call Stan from R
> out <- stan("GLMM_Poisson.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, iter = ni, warmup = nb, thin = nt,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=5348 on localhost:11688 at 19:43:57.623
starting worker pid=5356 on localhost:11688 at 19:43:57.748
starting worker pid=5364 on localhost:11688 at 19:43:57.872
starting worker pid=5372 on localhost:11688 at 19:43:58.001

SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 1).

Chain 1, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 1, Iteration:  1500 / 15000 [ 10%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 2).

Chain 2, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 1, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 2, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 1, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 1, Iteration:  5001 / 15000 [ 33%]  (Sampling)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 3).

Chain 3, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 2, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 2, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 2, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 3, Iteration:  1500 / 15000 [ 10%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson' NOW (CHAIN 4).

Chain 4, Iteration:     1 / 15000 [  0%]  (Warmup)
Chain 1, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 3, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 4, Iteration:  1500 / 15000 [ 10%]  (Warmup)
Chain 2, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 3, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 1, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 3, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 4, Iteration:  3000 / 15000 [ 20%]  (Warmup)
Chain 2, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 4, Iteration:  4500 / 15000 [ 30%]  (Warmup)
Chain 3, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 4, Iteration:  5001 / 15000 [ 33%]  (Sampling)
Chain 2, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 1, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 3, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 4, Iteration:  6500 / 15000 [ 43%]  (Sampling)
Chain 2, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 3, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 1, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 4, Iteration:  8000 / 15000 [ 53%]  (Sampling)
Chain 2, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 3, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 2, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 1, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 4, Iteration:  9500 / 15000 [ 63%]  (Sampling)
Chain 2, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.04306 seconds (Warm-up)
#                2.78251 seconds (Sampling)
#                3.82557 seconds (Total)


Chain 3, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 4, Iteration: 11000 / 15000 [ 73%]  (Sampling)
Chain 1, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 3, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 1, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.09131 seconds (Warm-up)
#                4.17431 seconds (Sampling)
#                5.26562 seconds (Total)


Chain 4, Iteration: 12500 / 15000 [ 83%]  (Sampling)
Chain 3, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.22066 seconds (Warm-up)
#                3.14068 seconds (Sampling)
#                4.36135 seconds (Total)


Chain 4, Iteration: 14000 / 15000 [ 93%]  (Sampling)
Chain 4, Iteration: 15000 / 15000 [100%]  (Sampling)
#  Elapsed Time: 1.11466 seconds (Warm-up)
#                3.40782 seconds (Sampling)
#                4.52248 seconds (Total)

> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLMM_Poisson.
4 chains, each with iter=15000; warmup=5000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

               mean se_mean    sd     2.5%      25%      50%      75%    97.5%
alpha          4.30    0.00  0.03     4.24     4.28     4.30     4.32     4.36
beta1          1.25    0.00  0.05     1.15     1.21     1.25     1.28     1.34
beta2          0.08    0.00  0.02     0.03     0.06     0.08     0.09     0.12
beta3         -0.23    0.00  0.03    -0.28    -0.24    -0.23    -0.21    -0.18
lambda[1]     32.43    0.06  3.44    25.86    30.05    32.33    34.70    39.38
lambda[2]     30.71    0.05  2.81    25.58    28.73    30.67    32.51    36.41
lambda[3]     29.19    0.04  2.38    24.78    27.51    29.11    30.73    34.02
lambda[4]     28.26    0.04  2.10    24.42    26.80    28.12    29.66    32.57
lambda[5]     27.18    0.03  1.85    23.53    25.95    27.19    28.44    30.76
lambda[6]     27.42    0.03  1.78    24.12    26.23    27.37    28.51    31.16
lambda[7]     27.29    0.03  1.68    24.12    26.17    27.29    28.36    30.66
lambda[8]     27.82    0.03  1.66    24.50    26.74    27.78    28.87    31.16
lambda[9]     28.85    0.03  1.71    25.47    27.76    28.81    29.87    32.40
lambda[10]    30.09    0.03  1.78    26.67    28.95    30.04    31.19    33.86
lambda[11]    32.10    0.03  1.96    28.66    30.80    31.98    33.23    36.28
lambda[12]    34.39    0.04  2.10    30.70    32.98    34.20    35.59    39.05
lambda[13]    36.32    0.04  2.12    32.37    34.96    36.24    37.55    40.87
lambda[14]    39.14    0.04  2.20    34.97    37.67    39.12    40.48    43.62
lambda[15]    42.84    0.04  2.39    38.39    41.28    42.75    44.24    48.01
lambda[16]    47.70    0.06  2.79    42.98    45.88    47.42    49.24    54.12
lambda[17]    51.96    0.05  2.86    46.73    50.07    51.78    53.61    58.26
lambda[18]    56.28    0.05  2.93    50.22    54.50    56.39    58.13    61.92
lambda[19]    63.07    0.05  3.14    56.92    61.06    63.01    64.96    69.54
lambda[20]    71.57    0.07  3.78    65.24    69.04    71.18    73.63    80.21
lambda[21]    75.77    0.08  3.99    66.73    73.60    76.16    78.37    82.55
lambda[22]    86.85    0.07  3.89    79.17    84.39    86.73    89.11    94.97
lambda[23]    95.89    0.07  4.28    86.63    93.41    95.97    98.47   104.41
lambda[24]   106.36    0.08  4.71    95.90   103.71   106.48   109.39   115.01
lambda[25]   119.58    0.08  5.06   109.48   116.36   119.39   122.58   130.29
lambda[26]   132.05    0.08  5.31   121.19   128.82   131.96   135.26   142.82
lambda[27]   146.61    0.10  6.04   135.68   142.76   146.26   150.05   159.78
lambda[28]   157.78    0.13  6.66   143.13   153.92   158.17   162.12   169.94
lambda[29]   168.62    0.21  8.36   149.27   163.82   169.69   174.42   182.65
lambda[30]   196.68    0.24  9.22   182.31   190.06   195.19   201.91   218.43
lambda[31]   205.61    0.13  7.95   190.54   200.46   205.32   210.51   222.28
lambda[32]   213.87    0.19  9.07   193.63   208.52   214.59   219.97   230.04
lambda[33]   236.57    0.20  9.39   220.80   230.11   235.47   241.93   257.84
lambda[34]   247.18    0.17  9.36   230.67   241.08   246.52   252.51   268.02
lambda[35]   255.45    0.15  9.01   238.04   249.69   254.96   261.03   274.25
lambda[36]   261.52    0.15  9.72   243.01   255.59   261.28   267.09   282.07
lambda[37]   262.79    0.15  9.66   242.42   257.10   262.91   268.92   281.22
lambda[38]   263.52    0.16  9.97   242.49   257.22   263.53   269.90   283.42
lambda[39]   261.90    0.18 11.02   241.39   254.69   261.36   268.77   284.29
lambda[40]   252.10    0.19 11.78   228.98   244.10   251.98   259.99   275.68
sigma          0.04    0.00  0.02     0.01     0.02     0.03     0.05     0.09
eps[1]        -0.01    0.00  0.04    -0.12    -0.03     0.00     0.01     0.07
eps[2]         0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.10
eps[3]         0.01    0.00  0.04    -0.08    -0.02     0.00     0.02     0.10
eps[4]         0.01    0.00  0.04    -0.07    -0.01     0.00     0.03     0.11
eps[5]        -0.01    0.00  0.04    -0.11    -0.03     0.00     0.01     0.07
eps[6]         0.01    0.00  0.04    -0.08    -0.01     0.00     0.03     0.10
eps[7]        -0.01    0.00  0.04    -0.10    -0.02     0.00     0.01     0.08
eps[8]        -0.01    0.00  0.04    -0.10    -0.03     0.00     0.01     0.07
eps[9]         0.00    0.00  0.04    -0.10    -0.02     0.00     0.02     0.08
eps[10]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.08
eps[11]        0.01    0.00  0.04    -0.07    -0.01     0.00     0.03     0.11
eps[12]        0.01    0.00  0.04    -0.06    -0.01     0.01     0.03     0.12
eps[13]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.08
eps[14]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.08
eps[15]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.09
eps[16]        0.02    0.00  0.04    -0.06    -0.01     0.01     0.04     0.13
eps[17]        0.01    0.00  0.04    -0.08    -0.01     0.00     0.03     0.10
eps[18]       -0.01    0.00  0.04    -0.11    -0.03     0.00     0.01     0.06
eps[19]        0.00    0.00  0.04    -0.09    -0.02     0.00     0.02     0.09
eps[20]        0.02    0.00  0.04    -0.05     0.00     0.01     0.04     0.13
eps[21]       -0.03    0.00  0.05    -0.15    -0.05    -0.02     0.00     0.04
eps[22]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.08
eps[23]       -0.01    0.00  0.04    -0.10    -0.03     0.00     0.01     0.07
eps[24]       -0.01    0.00  0.04    -0.10    -0.03     0.00     0.01     0.06
eps[25]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.08
eps[26]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.08
eps[27]        0.01    0.00  0.04    -0.07    -0.01     0.00     0.02     0.09
eps[28]       -0.01    0.00  0.04    -0.10    -0.03    -0.01     0.01     0.05
eps[29]       -0.04    0.00  0.05    -0.15    -0.06    -0.03     0.00     0.03
eps[30]        0.04    0.00  0.04    -0.02     0.00     0.03     0.06     0.15
eps[31]        0.01    0.00  0.03    -0.06    -0.01     0.00     0.02     0.09
eps[32]       -0.02    0.00  0.04    -0.11    -0.04    -0.01     0.00     0.04
eps[33]        0.02    0.00  0.04    -0.04     0.00     0.01     0.04     0.11
eps[34]        0.01    0.00  0.03    -0.05    -0.01     0.01     0.03     0.10
eps[35]        0.01    0.00  0.03    -0.05    -0.01     0.01     0.03     0.08
eps[36]        0.01    0.00  0.03    -0.06    -0.01     0.00     0.02     0.08
eps[37]       -0.01    0.00  0.03    -0.08    -0.02     0.00     0.01     0.06
eps[38]        0.00    0.00  0.03    -0.08    -0.02     0.00     0.01     0.06
eps[39]        0.00    0.00  0.04    -0.08    -0.02     0.00     0.02     0.07
eps[40]       -0.01    0.00  0.04    -0.10    -0.03    -0.01     0.01     0.06
lp__       18284.31    1.36 26.77 18246.19 18264.58 18278.66 18299.35 18350.87
           n_eff Rhat
alpha       2884 1.00
beta1       2563 1.00
beta2       2708 1.00
beta3       2778 1.00
lambda[1]   2862 1.00
lambda[2]   3004 1.00
lambda[3]   3218 1.00
lambda[4]   3289 1.00
lambda[5]   3027 1.00
lambda[6]   3525 1.00
lambda[7]   3275 1.00
lambda[8]   3565 1.00
lambda[9]   3523 1.00
lambda[10]  3543 1.00
lambda[11]  3167 1.00
lambda[12]  2780 1.00
lambda[13]  3556 1.00
lambda[14]  3451 1.00
lambda[15]  3331 1.00
lambda[16]  2483 1.00
lambda[17]  3348 1.00
lambda[18]  3339 1.00
lambda[19]  3308 1.00
lambda[20]  2610 1.00
lambda[21]  2432 1.00
lambda[22]  3172 1.00
lambda[23]  3619 1.00
lambda[24]  3253 1.00
lambda[25]  3630 1.00
lambda[26]  3918 1.00
lambda[27]  3520 1.00
lambda[28]  2718 1.00
lambda[29]  1601 1.00
lambda[30]  1478 1.00
lambda[31]  3687 1.00
lambda[32]  2226 1.00
lambda[33]  2232 1.00
lambda[34]  2923 1.00
lambda[35]  3844 1.00
lambda[36]  3955 1.00
lambda[37]  4000 1.00
lambda[38]  3848 1.00
lambda[39]  3814 1.00
lambda[40]  4000 1.00
sigma        693 1.00
eps[1]      3774 1.00
eps[2]      4000 1.00
eps[3]      4000 1.00
eps[4]      4000 1.00
eps[5]      3616 1.00
eps[6]      3887 1.00
eps[7]      4000 1.00
eps[8]      4000 1.00
eps[9]      3985 1.00
eps[10]     4000 1.00
eps[11]     3605 1.00
eps[12]     3263 1.00
eps[13]     4000 1.00
eps[14]     3874 1.00
eps[15]     3780 1.00
eps[16]     2847 1.00
eps[17]     4000 1.00
eps[18]     3645 1.00
eps[19]     3341 1.00
eps[20]     2506 1.00
eps[21]     2063 1.00
eps[22]     4000 1.00
eps[23]     4000 1.00
eps[24]     3291 1.00
eps[25]     4000 1.00
eps[26]     3832 1.00
eps[27]     3849 1.00
eps[28]     3005 1.00
eps[29]     1674 1.00
eps[30]     1476 1.00
eps[31]     4000 1.00
eps[32]     2324 1.00
eps[33]     1990 1.00
eps[34]     2694 1.00
eps[35]     3734 1.00
eps[36]     3811 1.00
eps[37]     3692 1.00
eps[38]     3556 1.00
eps[39]     3695 1.00
eps[40]     3177 1.00
lp__         389 1.01

Samples were drawn using NUTS(diag_e) at Wed Dec 16 19:44:05 2015.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   user  system elapsed 
  1.566   0.186   9.499 
