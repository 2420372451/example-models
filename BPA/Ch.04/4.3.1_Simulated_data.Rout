
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ## 4. Introduction to random effects: Conventional Poisson GLMM for
> ## count data
> ## 4.3. Mixed models with random effects for variability among groups
> ## (site and year effects)
> ## 4.3.1. Generation and analysis of simulated data
> 
> library(rstan)
Loading required package: ggplot2
rstan (Version 2.8.1, packaged: 2015-11-18 17:18:35 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> set.seed(123)
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLMM_Poisson2.data.R")
> 
> ## Parameters monitored
> params <- c("mu", "alpha", "beta", "sd_alpha", "sd_year")
> 
> # MCMC settings
> ni <- 40000
> nt <- 10
> nb <- 30000
> nc <- 4
> 
> ## Initial values
> inits <- lapply(1:nc, function(i) {
+     list(mu = runif(1, 0, 2),
+          alpha = runif(stan_data$nsite, -1, 1),
+          beta = runif(3, -1, 1),
+          sd_alpha = runif(1, 0, 0.1),
+          sd_year = runif(1, 0, 0.1))})
> 
> ## Call Stan from R
> out <- stan("GLMM_Poisson2.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, iter = ni, warmup = nb, thin = nt,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=5398 on localhost:11647 at 19:44:07.637
starting worker pid=5406 on localhost:11647 at 19:44:07.761
starting worker pid=5414 on localhost:11647 at 19:44:07.887
starting worker pid=5422 on localhost:11647 at 19:44:08.017

SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 1).

Chain 1, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 2).

Chain 2, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 3).

Chain 3, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 4).

Chain 4, Iteration:     1 / 40000 [  0%]  (Warmup)
Chain 1, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 4, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 3, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 2, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 3, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 3, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 1, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 4, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 3, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 2, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 3, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 3, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 1, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 3, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 3, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 1, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 4, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 1, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 3, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 1, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 1, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 1, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 2, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 4, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 3, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 1, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 4, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 2, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 3, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1286.19 seconds (Warm-up)
#                1056.37 seconds (Sampling)
#                2342.56 seconds (Total)


Chain 1, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 2, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 1, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1939.94 seconds (Warm-up)
#                524.849 seconds (Sampling)
#                2464.79 seconds (Total)


Chain 4, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 2, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 2, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 4, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 2, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 4, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 4, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 2, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 4, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 4, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 2825.79 seconds (Warm-up)
#                483.122 seconds (Sampling)
#                3308.91 seconds (Total)


Chain 2, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 2, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 2761.57 seconds (Warm-up)
#                978.956 seconds (Sampling)
#                3740.52 seconds (Total)

> 
> ## Note: this model converges very slowly, and there may be
> ## transitions after warmup that exceed the maximum treedepth.
> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLMM_Poisson2.
4 chains, each with iter=40000; warmup=30000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

                 mean se_mean   sd       2.5%        25%        50%        75%
mu               4.17    0.01 0.05       4.07       4.14       4.17       4.21
alpha[1]         3.97    0.01 0.05       3.86       3.93       3.97       4.00
alpha[2]         4.10    0.01 0.05       4.00       4.06       4.10       4.13
alpha[3]         4.62    0.01 0.05       4.52       4.59       4.62       4.65
alpha[4]         4.16    0.01 0.05       4.06       4.13       4.16       4.19
alpha[5]         4.16    0.01 0.05       4.06       4.13       4.16       4.19
alpha[6]         4.66    0.01 0.05       4.56       4.62       4.66       4.69
alpha[7]         4.29    0.01 0.05       4.19       4.26       4.29       4.33
alpha[8]         3.78    0.01 0.05       3.67       3.74       3.78       3.81
alpha[9]         3.93    0.01 0.05       3.83       3.90       3.93       3.96
alpha[10]        3.99    0.01 0.05       3.89       3.95       3.99       4.02
alpha[11]        4.52    0.01 0.05       4.43       4.49       4.52       4.56
alpha[12]        4.26    0.01 0.05       4.16       4.23       4.26       4.29
alpha[13]        4.28    0.01 0.05       4.17       4.24       4.28       4.31
alpha[14]        4.19    0.01 0.05       4.10       4.16       4.19       4.23
alpha[15]        4.01    0.01 0.05       3.91       3.98       4.01       4.05
alpha[16]        4.69    0.01 0.05       4.60       4.66       4.69       4.72
alpha[17]        4.30    0.01 0.05       4.20       4.27       4.30       4.33
alpha[18]        3.55    0.01 0.05       3.44       3.51       3.55       3.58
alpha[19]        4.36    0.01 0.05       4.26       4.33       4.36       4.39
alpha[20]        3.99    0.01 0.05       3.88       3.95       3.99       4.02
alpha[21]        3.80    0.01 0.05       3.70       3.77       3.80       3.83
alpha[22]        4.07    0.01 0.05       3.97       4.04       4.07       4.11
alpha[23]        3.85    0.01 0.05       3.75       3.82       3.85       3.89
alpha[24]        3.93    0.01 0.05       3.83       3.90       3.93       3.96
alpha[25]        3.96    0.01 0.05       3.86       3.93       3.96       3.99
alpha[26]        3.64    0.01 0.05       3.54       3.61       3.64       3.68
alpha[27]        4.40    0.01 0.05       4.30       4.36       4.40       4.43
alpha[28]        4.20    0.01 0.05       4.10       4.17       4.20       4.24
alpha[29]        3.82    0.01 0.05       3.72       3.79       3.82       3.86
alpha[30]        4.54    0.01 0.05       4.44       4.50       4.54       4.57
alpha[31]        4.27    0.01 0.05       4.17       4.24       4.27       4.31
alpha[32]        4.04    0.01 0.05       3.94       4.01       4.04       4.07
alpha[33]        4.42    0.01 0.05       4.32       4.39       4.42       4.45
alpha[34]        4.43    0.01 0.05       4.33       4.40       4.43       4.46
alpha[35]        4.39    0.01 0.05       4.29       4.36       4.39       4.42
alpha[36]        4.34    0.01 0.05       4.24       4.30       4.34       4.37
alpha[37]        4.30    0.01 0.05       4.20       4.27       4.30       4.34
alpha[38]        4.14    0.01 0.05       4.04       4.11       4.14       4.17
alpha[39]        4.05    0.01 0.05       3.95       4.02       4.06       4.09
alpha[40]        4.03    0.01 0.05       3.93       4.00       4.03       4.06
alpha[41]        3.93    0.01 0.05       3.83       3.90       3.93       3.97
alpha[42]        4.09    0.01 0.05       3.99       4.06       4.09       4.13
alpha[43]        3.72    0.01 0.05       3.62       3.69       3.73       3.76
alpha[44]        4.79    0.01 0.05       4.70       4.76       4.79       4.83
alpha[45]        4.54    0.01 0.05       4.44       4.50       4.54       4.57
alpha[46]        3.75    0.01 0.05       3.65       3.72       3.75       3.79
alpha[47]        4.02    0.01 0.05       3.92       3.98       4.02       4.05
alpha[48]        4.04    0.01 0.05       3.94       4.01       4.04       4.07
alpha[49]        4.37    0.01 0.05       4.27       4.33       4.37       4.40
alpha[50]        4.12    0.01 0.05       4.02       4.09       4.12       4.16
alpha[51]        4.21    0.01 0.05       4.11       4.17       4.21       4.24
alpha[52]        4.12    0.01 0.05       4.02       4.09       4.12       4.15
alpha[53]        4.16    0.01 0.05       4.06       4.13       4.16       4.19
alpha[54]        4.55    0.01 0.05       4.45       4.52       4.55       4.58
alpha[55]        4.07    0.01 0.05       3.97       4.03       4.07       4.10
alpha[56]        4.60    0.01 0.05       4.50       4.56       4.60       4.63
alpha[57]        3.70    0.01 0.05       3.59       3.66       3.70       3.73
alpha[58]        4.34    0.01 0.05       4.24       4.31       4.34       4.37
alpha[59]        4.17    0.01 0.05       4.07       4.13       4.17       4.20
alpha[60]        4.22    0.01 0.05       4.12       4.19       4.22       4.25
alpha[61]        4.24    0.01 0.05       4.14       4.20       4.24       4.27
alpha[62]        4.02    0.01 0.05       3.91       3.98       4.02       4.05
alpha[63]        4.08    0.01 0.05       3.98       4.05       4.08       4.11
alpha[64]        3.85    0.01 0.05       3.75       3.82       3.85       3.88
alpha[65]        3.86    0.01 0.05       3.76       3.83       3.86       3.90
alpha[66]        4.21    0.01 0.05       4.11       4.18       4.21       4.24
alpha[67]        4.28    0.01 0.05       4.18       4.25       4.28       4.31
alpha[68]        4.17    0.01 0.05       4.07       4.14       4.17       4.20
alpha[69]        4.40    0.01 0.05       4.30       4.37       4.40       4.43
alpha[70]        4.77    0.01 0.05       4.67       4.74       4.77       4.80
alpha[71]        4.00    0.01 0.05       3.90       3.97       4.00       4.04
alpha[72]        3.47    0.01 0.05       3.36       3.43       3.47       3.50
alpha[73]        4.43    0.01 0.05       4.34       4.40       4.43       4.46
alpha[74]        3.92    0.01 0.05       3.82       3.89       3.92       3.95
alpha[75]        3.95    0.01 0.05       3.85       3.92       3.95       3.98
alpha[76]        4.44    0.01 0.05       4.34       4.41       4.44       4.48
alpha[77]        4.07    0.01 0.05       3.97       4.04       4.08       4.11
alpha[78]        3.79    0.01 0.05       3.68       3.75       3.79       3.82
alpha[79]        4.22    0.01 0.05       4.12       4.19       4.22       4.26
alpha[80]        4.11    0.01 0.05       4.00       4.07       4.11       4.14
alpha[81]        4.15    0.01 0.05       4.05       4.12       4.15       4.18
alpha[82]        4.25    0.01 0.05       4.15       4.22       4.25       4.28
alpha[83]        4.02    0.01 0.05       3.93       3.99       4.02       4.06
alpha[84]        4.36    0.01 0.05       4.26       4.33       4.36       4.39
alpha[85]        4.10    0.01 0.05       4.00       4.07       4.10       4.14
alpha[86]        4.25    0.01 0.05       4.15       4.22       4.25       4.28
alpha[87]        4.47    0.01 0.05       4.37       4.44       4.47       4.50
alpha[88]        4.26    0.01 0.05       4.16       4.23       4.26       4.29
alpha[89]        4.04    0.01 0.05       3.94       4.01       4.04       4.07
alpha[90]        4.49    0.01 0.05       4.39       4.45       4.49       4.52
alpha[91]        4.43    0.01 0.05       4.33       4.40       4.43       4.46
alpha[92]        4.32    0.01 0.05       4.22       4.29       4.32       4.35
alpha[93]        4.25    0.01 0.05       4.15       4.22       4.25       4.28
alpha[94]        3.95    0.01 0.05       3.85       3.92       3.95       3.98
alpha[95]        4.54    0.01 0.05       4.44       4.51       4.54       4.57
alpha[96]        3.94    0.01 0.05       3.84       3.91       3.94       3.97
alpha[97]        4.79    0.01 0.05       4.69       4.76       4.79       4.82
alpha[98]        4.62    0.01 0.05       4.52       4.59       4.62       4.65
alpha[99]        4.08    0.01 0.05       3.97       4.04       4.08       4.11
alpha[100]       3.85    0.01 0.05       3.75       3.81       3.85       3.88
beta[1]          2.00    0.01 0.13       1.74       1.91       2.00       2.09
beta[2]          0.09    0.01 0.11      -0.12       0.02       0.10       0.16
beta[3]         -1.24    0.01 0.20      -1.63      -1.37      -1.23      -1.10
sd_alpha         0.28    0.00 0.02       0.24       0.26       0.28       0.29
sd_year          0.19    0.00 0.02       0.15       0.18       0.19       0.21
lp__       1442833.93    0.15 8.57 1442816.13 1442828.29 1442834.36 1442839.99
                97.5% n_eff Rhat
mu               4.28   103 1.02
alpha[1]         4.07    88 1.03
alpha[2]         4.19    86 1.02
alpha[3]         4.71    81 1.03
alpha[4]         4.26    83 1.03
alpha[5]         4.26    85 1.03
alpha[6]         4.75    81 1.03
alpha[7]         4.39    83 1.03
alpha[8]         3.88    87 1.02
alpha[9]         4.03    85 1.03
alpha[10]        4.09    87 1.02
alpha[11]        4.62    82 1.03
alpha[12]        4.35    83 1.03
alpha[13]        4.37    83 1.03
alpha[14]        4.29    82 1.03
alpha[15]        4.11    87 1.03
alpha[16]        4.79    80 1.03
alpha[17]        4.40    83 1.03
alpha[18]        3.65    91 1.03
alpha[19]        4.46    84 1.03
alpha[20]        4.08    84 1.03
alpha[21]        3.90    88 1.02
alpha[22]        4.17    83 1.03
alpha[23]        3.95    90 1.02
alpha[24]        4.03    87 1.03
alpha[25]        4.06    87 1.02
alpha[26]        3.75    90 1.02
alpha[27]        4.49    82 1.03
alpha[28]        4.30    83 1.03
alpha[29]        3.92    88 1.02
alpha[30]        4.63    81 1.03
alpha[31]        4.37    85 1.03
alpha[32]        4.14    86 1.03
alpha[33]        4.52    83 1.03
alpha[34]        4.52    83 1.03
alpha[35]        4.49    82 1.03
alpha[36]        4.43    82 1.03
alpha[37]        4.40    82 1.03
alpha[38]        4.24    85 1.03
alpha[39]        4.15    86 1.03
alpha[40]        4.13    87 1.03
alpha[41]        4.03    87 1.03
alpha[42]        4.19    86 1.03
alpha[43]        3.83    89 1.03
alpha[44]        4.89    81 1.03
alpha[45]        4.63    82 1.03
alpha[46]        3.86    87 1.02
alpha[47]        4.11    86 1.03
alpha[48]        4.14    86 1.03
alpha[49]        4.46    83 1.03
alpha[50]        4.22    84 1.03
alpha[51]        4.30    84 1.03
alpha[52]        4.21    85 1.03
alpha[53]        4.26    85 1.03
alpha[54]        4.65    81 1.03
alpha[55]        4.17    86 1.03
alpha[56]        4.69    82 1.03
alpha[57]        3.80    92 1.02
alpha[58]        4.44    82 1.03
alpha[59]        4.26    87 1.02
alpha[60]        4.32    82 1.03
alpha[61]        4.33    82 1.03
alpha[62]        4.11    85 1.02
alpha[63]        4.18    86 1.03
alpha[64]        3.95    89 1.02
alpha[65]        3.96    87 1.03
alpha[66]        4.30    87 1.03
alpha[67]        4.38    84 1.03
alpha[68]        4.27    86 1.03
alpha[69]        4.50    84 1.03
alpha[70]        4.87    79 1.03
alpha[71]        4.10    86 1.03
alpha[72]        3.57    94 1.02
alpha[73]        4.53    82 1.03
alpha[74]        4.02    87 1.03
alpha[75]        4.05    86 1.03
alpha[76]        4.54    82 1.03
alpha[77]        4.17    84 1.03
alpha[78]        3.88    88 1.03
alpha[79]        4.32    84 1.03
alpha[80]        4.21    85 1.03
alpha[81]        4.24    87 1.02
alpha[82]        4.35    82 1.03
alpha[83]        4.13    86 1.03
alpha[84]        4.46    83 1.03
alpha[85]        4.20    82 1.03
alpha[86]        4.35    81 1.03
alpha[87]        4.57    83 1.03
alpha[88]        4.36    83 1.03
alpha[89]        4.14    87 1.02
alpha[90]        4.58    82 1.03
alpha[91]        4.53    81 1.03
alpha[92]        4.41    83 1.03
alpha[93]        4.34    83 1.03
alpha[94]        4.05    86 1.03
alpha[95]        4.64    83 1.02
alpha[96]        4.04    88 1.02
alpha[97]        4.89    80 1.03
alpha[98]        4.72    82 1.03
alpha[99]        4.17    84 1.03
alpha[100]       3.95    88 1.03
beta[1]          2.25   304 1.01
beta[2]          0.31   128 1.03
beta[3]         -0.84   375 1.01
sd_alpha         0.32  1611 1.00
sd_year          0.24  1732 1.00
lp__       1442849.75  3450 1.00

Samples were drawn using NUTS(diag_e) at Wed Dec 16 20:46:45 2015.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
    user   system  elapsed 
   1.980    0.211 3759.806 
