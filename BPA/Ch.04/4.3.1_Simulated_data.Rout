
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R は、自由なソフトウェアであり、「完全に無保証」です。 
一定の条件に従えば、自由にこれを再配布することができます。 
配布条件の詳細に関しては、'license()' あるいは 'licence()' と入力してください。 

R は多くの貢献者による共同プロジェクトです。 
詳しくは 'contributors()' と入力してください。 
また、R や R のパッケージを出版物で引用する際の形式については 
'citation()' と入力してください。 

'demo()' と入力すればデモをみることができます。 
'help()' とすればオンラインヘルプが出ます。 
'help.start()' で HTML ブラウザによるヘルプがみられます。 
'q()' と入力すれば R を終了します。 

 [以前にセーブされたワークスペースを復帰します] 

> ## 4. Introduction to random effects: Conventional Poisson GLMM for
> ## count data
> ## 4.3. Mixed models with random effects for variability among groups
> ## (site and year effects)
> ## 4.3.1. Generation and analysis of simulated data
> 
> library(rstan)
 要求されたパッケージ ggplot2 をロード中です 
rstan (Version 2.8.2, packaged: 2015-11-26 15:27:02 UTC, GitRev: 05c3d0058b6a)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())
> set.seed(123)
> 
> ## Read data
> ## The data generation code is in bpa-code.txt, available at
> ## http://www.vogelwarte.ch/de/projekte/publikationen/bpa/complete-code-and-data-files-of-the-book.html
> stan_data <- read_rdump("GLMM_Poisson2.data.R")
> 
> ## Parameters monitored
> params <- c("mu", "alpha", "beta", "sd_alpha", "sd_year")
> 
> # MCMC settings
> ni <- 40000
> nt <- 10
> nb <- 30000
> nc <- 4
> 
> ## Initial values
> inits <- lapply(1:nc, function(i) {
+     list(mu = runif(1, 0, 2),
+          alpha = runif(stan_data$nsite, -1, 1),
+          beta = runif(3, -1, 1),
+          sd_alpha = runif(1, 0, 0.1),
+          sd_year = runif(1, 0, 0.1))})
> 
> ## Call Stan from R
> out <- stan("GLMM_Poisson2.stan", data = stan_data,
+             init = inits, pars = params,
+             chains = nc, iter = ni, warmup = nb, thin = nt,
+             seed = 1,
+             open_progress = FALSE)
starting worker pid=7258 on localhost:11902 at 20:09:11.220
starting worker pid=7266 on localhost:11902 at 20:09:11.381
starting worker pid=7274 on localhost:11902 at 20:09:11.536
starting worker pid=7282 on localhost:11902 at 20:09:11.693

SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 1).

Chain 1, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 2).

Chain 2, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 3).

Chain 3, Iteration:     1 / 40000 [  0%]  (Warmup)
SAMPLING FOR MODEL 'GLMM_Poisson2' NOW (CHAIN 4).

Chain 4, Iteration:     1 / 40000 [  0%]  (Warmup)
Chain 1, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 4, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 3, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 2, Iteration:  4000 / 40000 [ 10%]  (Warmup)
Chain 4, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 1, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 3, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 2, Iteration:  8000 / 40000 [ 20%]  (Warmup)
Chain 2, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 4, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 1, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 3, Iteration: 12000 / 40000 [ 30%]  (Warmup)
Chain 2, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 1, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 4, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 3, Iteration: 16000 / 40000 [ 40%]  (Warmup)
Chain 2, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 1, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 3, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 2, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 4, Iteration: 20000 / 40000 [ 50%]  (Warmup)
Chain 1, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 3, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 2, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 1, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 2, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 4, Iteration: 24000 / 40000 [ 60%]  (Warmup)
Chain 1, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 3, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 3, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 4, Iteration: 28000 / 40000 [ 70%]  (Warmup)
Chain 2, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 4, Iteration: 30001 / 40000 [ 75%]  (Sampling)
Chain 1, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 3, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 2, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 4, Iteration: 34000 / 40000 [ 85%]  (Sampling)
Chain 1, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 2, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1168.94 seconds (Warm-up)
#                592.954 seconds (Sampling)
#                1761.9 seconds (Total)


Chain 3, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 1, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1205.27 seconds (Warm-up)
#                592.696 seconds (Sampling)
#                1797.97 seconds (Total)


Chain 3, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1290.5 seconds (Warm-up)
#                581.07 seconds (Sampling)
#                1871.57 seconds (Total)


Chain 4, Iteration: 38000 / 40000 [ 95%]  (Sampling)
Chain 4, Iteration: 40000 / 40000 [100%]  (Sampling)
#  Elapsed Time: 1420.69 seconds (Warm-up)
#                580.376 seconds (Sampling)
#                2001.06 seconds (Total)

> 
> ## Note: this model converges very slowly, and there may be
> ## transitions after warmup that exceed the maximum treedepth.
> 
> ## Summarize posteriors
> print(out)
Inference for Stan model: GLMM_Poisson2.
4 chains, each with iter=40000; warmup=30000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

                 mean se_mean   sd       2.5%        25%        50%        75%
mu               4.18    0.01 0.05       4.08       4.15       4.18       4.21
alpha[1]         3.97    0.01 0.04       3.88       3.94       3.97       4.00
alpha[2]         4.10    0.01 0.05       4.01       4.07       4.10       4.13
alpha[3]         4.62    0.01 0.04       4.53       4.59       4.62       4.65
alpha[4]         4.16    0.01 0.05       4.07       4.13       4.16       4.19
alpha[5]         4.16    0.01 0.05       4.07       4.13       4.16       4.19
alpha[6]         4.66    0.01 0.04       4.57       4.63       4.66       4.69
alpha[7]         4.30    0.01 0.05       4.21       4.27       4.30       4.33
alpha[8]         3.78    0.01 0.05       3.69       3.75       3.78       3.81
alpha[9]         3.94    0.01 0.05       3.84       3.91       3.94       3.97
alpha[10]        3.99    0.01 0.05       3.90       3.96       3.99       4.02
alpha[11]        4.53    0.01 0.04       4.44       4.50       4.53       4.56
alpha[12]        4.26    0.01 0.04       4.17       4.23       4.26       4.29
alpha[13]        4.28    0.01 0.04       4.19       4.25       4.28       4.31
alpha[14]        4.20    0.01 0.04       4.11       4.17       4.20       4.23
alpha[15]        4.02    0.01 0.05       3.92       3.99       4.02       4.05
alpha[16]        4.70    0.01 0.04       4.61       4.67       4.70       4.72
alpha[17]        4.31    0.01 0.04       4.22       4.28       4.31       4.34
alpha[18]        3.55    0.01 0.05       3.46       3.52       3.55       3.58
alpha[19]        4.37    0.01 0.04       4.28       4.34       4.37       4.40
alpha[20]        3.99    0.01 0.05       3.90       3.96       3.99       4.02
alpha[21]        3.80    0.01 0.05       3.71       3.77       3.80       3.83
alpha[22]        4.08    0.01 0.05       3.99       4.05       4.08       4.11
alpha[23]        3.86    0.01 0.05       3.77       3.83       3.86       3.89
alpha[24]        3.94    0.01 0.05       3.85       3.91       3.94       3.97
alpha[25]        3.97    0.01 0.05       3.87       3.93       3.96       4.00
alpha[26]        3.65    0.01 0.05       3.55       3.62       3.65       3.68
alpha[27]        4.40    0.01 0.04       4.31       4.37       4.40       4.43
alpha[28]        4.21    0.01 0.05       4.11       4.18       4.21       4.24
alpha[29]        3.83    0.01 0.05       3.73       3.80       3.83       3.86
alpha[30]        4.54    0.01 0.04       4.45       4.51       4.54       4.57
alpha[31]        4.28    0.01 0.04       4.19       4.25       4.28       4.31
alpha[32]        4.05    0.01 0.05       3.96       4.02       4.05       4.08
alpha[33]        4.42    0.01 0.04       4.33       4.39       4.43       4.45
alpha[34]        4.43    0.01 0.04       4.35       4.40       4.43       4.46
alpha[35]        4.39    0.01 0.04       4.30       4.36       4.39       4.42
alpha[36]        4.34    0.01 0.04       4.25       4.31       4.34       4.37
alpha[37]        4.31    0.01 0.04       4.22       4.28       4.31       4.34
alpha[38]        4.15    0.01 0.05       4.05       4.12       4.15       4.18
alpha[39]        4.06    0.01 0.05       3.97       4.03       4.06       4.09
alpha[40]        4.03    0.01 0.05       3.94       4.00       4.03       4.06
alpha[41]        3.94    0.01 0.05       3.85       3.91       3.94       3.97
alpha[42]        4.10    0.01 0.04       4.01       4.07       4.10       4.13
alpha[43]        3.73    0.01 0.05       3.63       3.70       3.73       3.76
alpha[44]        4.80    0.01 0.04       4.71       4.77       4.80       4.83
alpha[45]        4.54    0.01 0.04       4.45       4.51       4.54       4.57
alpha[46]        3.76    0.01 0.05       3.66       3.73       3.76       3.79
alpha[47]        4.02    0.01 0.05       3.93       3.99       4.02       4.05
alpha[48]        4.05    0.01 0.05       3.95       4.02       4.05       4.08
alpha[49]        4.37    0.01 0.04       4.28       4.34       4.37       4.40
alpha[50]        4.13    0.01 0.04       4.04       4.10       4.13       4.16
alpha[51]        4.21    0.01 0.05       4.12       4.18       4.21       4.24
alpha[52]        4.12    0.01 0.04       4.03       4.09       4.12       4.15
alpha[53]        4.17    0.01 0.04       4.08       4.14       4.16       4.20
alpha[54]        4.55    0.01 0.04       4.47       4.53       4.55       4.58
alpha[55]        4.07    0.01 0.04       3.98       4.04       4.07       4.10
alpha[56]        4.60    0.01 0.04       4.51       4.57       4.60       4.63
alpha[57]        3.70    0.01 0.05       3.61       3.67       3.70       3.73
alpha[58]        4.35    0.01 0.04       4.26       4.32       4.35       4.37
alpha[59]        4.17    0.01 0.05       4.08       4.14       4.17       4.20
alpha[60]        4.23    0.01 0.04       4.13       4.20       4.23       4.26
alpha[61]        4.24    0.01 0.04       4.15       4.21       4.24       4.27
alpha[62]        4.02    0.01 0.05       3.93       3.99       4.02       4.05
alpha[63]        4.08    0.01 0.05       3.99       4.05       4.08       4.11
alpha[64]        3.86    0.01 0.05       3.76       3.83       3.86       3.89
alpha[65]        3.87    0.01 0.05       3.78       3.84       3.87       3.90
alpha[66]        4.21    0.01 0.04       4.12       4.18       4.21       4.24
alpha[67]        4.29    0.01 0.04       4.19       4.26       4.29       4.32
alpha[68]        4.17    0.01 0.04       4.08       4.15       4.17       4.20
alpha[69]        4.40    0.01 0.04       4.31       4.37       4.40       4.43
alpha[70]        4.77    0.01 0.04       4.68       4.75       4.77       4.80
alpha[71]        4.01    0.00 0.05       3.92       3.98       4.01       4.04
alpha[72]        3.47    0.01 0.05       3.38       3.44       3.47       3.50
alpha[73]        4.44    0.01 0.04       4.35       4.41       4.44       4.47
alpha[74]        3.92    0.01 0.05       3.83       3.89       3.93       3.95
alpha[75]        3.95    0.01 0.05       3.86       3.92       3.95       3.98
alpha[76]        4.45    0.01 0.04       4.36       4.42       4.45       4.48
alpha[77]        4.08    0.01 0.04       3.99       4.05       4.08       4.11
alpha[78]        3.79    0.01 0.05       3.70       3.76       3.79       3.82
alpha[79]        4.23    0.01 0.04       4.14       4.20       4.23       4.26
alpha[80]        4.11    0.01 0.05       4.02       4.08       4.11       4.14
alpha[81]        4.15    0.01 0.04       4.06       4.12       4.15       4.18
alpha[82]        4.26    0.01 0.04       4.17       4.23       4.26       4.29
alpha[83]        4.03    0.01 0.05       3.94       4.00       4.03       4.06
alpha[84]        4.36    0.01 0.04       4.27       4.33       4.36       4.39
alpha[85]        4.11    0.01 0.05       4.02       4.08       4.11       4.14
alpha[86]        4.25    0.01 0.04       4.16       4.22       4.25       4.28
alpha[87]        4.47    0.01 0.04       4.39       4.45       4.47       4.50
alpha[88]        4.27    0.01 0.04       4.18       4.24       4.27       4.30
alpha[89]        4.05    0.01 0.05       3.96       4.02       4.05       4.08
alpha[90]        4.49    0.01 0.04       4.40       4.46       4.49       4.52
alpha[91]        4.44    0.01 0.04       4.35       4.41       4.44       4.46
alpha[92]        4.32    0.01 0.04       4.23       4.29       4.32       4.35
alpha[93]        4.25    0.01 0.04       4.16       4.22       4.25       4.28
alpha[94]        3.96    0.01 0.05       3.87       3.93       3.96       3.99
alpha[95]        4.55    0.01 0.04       4.46       4.52       4.55       4.58
alpha[96]        3.95    0.01 0.05       3.85       3.92       3.95       3.98
alpha[97]        4.80    0.01 0.04       4.71       4.77       4.80       4.82
alpha[98]        4.62    0.01 0.04       4.54       4.60       4.62       4.65
alpha[99]        4.08    0.01 0.05       3.99       4.05       4.08       4.11
alpha[100]       3.85    0.01 0.05       3.76       3.82       3.85       3.88
beta[1]          2.00    0.01 0.13       1.75       1.91       2.00       2.09
beta[2]          0.08    0.01 0.10      -0.12       0.02       0.08       0.15
beta[3]         -1.24    0.01 0.20      -1.63      -1.37      -1.24      -1.10
sd_alpha         0.28    0.00 0.02       0.24       0.26       0.28       0.29
sd_year          0.19    0.00 0.02       0.15       0.17       0.19       0.21
lp__       1442833.88    0.14 8.52 1442816.04 1442828.26 1442834.30 1442839.82
                97.5% n_eff Rhat
mu               4.28    96 1.02
alpha[1]         4.06    79 1.03
alpha[2]         4.19    74 1.02
alpha[3]         4.71    73 1.02
alpha[4]         4.25    76 1.02
alpha[5]         4.25    76 1.03
alpha[6]         4.74    75 1.02
alpha[7]         4.39    77 1.02
alpha[8]         3.87    77 1.02
alpha[9]         4.03    78 1.02
alpha[10]        4.08    79 1.02
alpha[11]        4.62    72 1.03
alpha[12]        4.35    77 1.02
alpha[13]        4.37    76 1.02
alpha[14]        4.28    74 1.03
alpha[15]        4.10    78 1.03
alpha[16]        4.78    70 1.03
alpha[17]        4.39    77 1.02
alpha[18]        3.65    85 1.02
alpha[19]        4.45    74 1.02
alpha[20]        4.08    78 1.02
alpha[21]        3.89    82 1.02
alpha[22]        4.16    78 1.02
alpha[23]        3.95    79 1.02
alpha[24]        4.03    79 1.03
alpha[25]        4.05    80 1.02
alpha[26]        3.74    82 1.02
alpha[27]        4.49    74 1.02
alpha[28]        4.30    76 1.02
alpha[29]        3.92    82 1.02
alpha[30]        4.63    71 1.03
alpha[31]        4.36    78 1.02
alpha[32]        4.13    78 1.02
alpha[33]        4.51    73 1.03
alpha[34]        4.52    72 1.03
alpha[35]        4.48    74 1.03
alpha[36]        4.43    72 1.02
alpha[37]        4.39    73 1.03
alpha[38]        4.24    80 1.02
alpha[39]        4.15    78 1.02
alpha[40]        4.12    78 1.03
alpha[41]        4.03    78 1.02
alpha[42]        4.19    75 1.03
alpha[43]        3.82    79 1.02
alpha[44]        4.88    70 1.03
alpha[45]        4.63    71 1.03
alpha[46]        3.85    80 1.02
alpha[47]        4.11    78 1.02
alpha[48]        4.13    80 1.02
alpha[49]        4.46    74 1.02
alpha[50]        4.22    74 1.03
alpha[51]        4.30    79 1.02
alpha[52]        4.21    74 1.03
alpha[53]        4.25    76 1.02
alpha[54]        4.64    72 1.03
alpha[55]        4.16    76 1.03
alpha[56]        4.69    72 1.03
alpha[57]        3.79    80 1.02
alpha[58]        4.43    78 1.02
alpha[59]        4.26    75 1.02
alpha[60]        4.31    76 1.02
alpha[61]        4.33    76 1.02
alpha[62]        4.11    80 1.02
alpha[63]        4.17    79 1.02
alpha[64]        3.95    80 1.02
alpha[65]        3.95    81 1.02
alpha[66]        4.30    73 1.03
alpha[67]        4.37    76 1.02
alpha[68]        4.26    76 1.02
alpha[69]        4.49    71 1.03
alpha[70]        4.86    71 1.03
alpha[71]        4.10    83 1.02
alpha[72]        3.56    83 1.02
alpha[73]        4.52    75 1.02
alpha[74]        4.01    77 1.02
alpha[75]        4.04    76 1.02
alpha[76]        4.53    74 1.02
alpha[77]        4.17    78 1.02
alpha[78]        3.88    80 1.02
alpha[79]        4.32    72 1.02
alpha[80]        4.20    77 1.02
alpha[81]        4.24    77 1.02
alpha[82]        4.34    78 1.02
alpha[83]        4.12    77 1.02
alpha[84]        4.45    76 1.02
alpha[85]        4.20    78 1.02
alpha[86]        4.34    72 1.03
alpha[87]        4.56    73 1.02
alpha[88]        4.35    74 1.02
alpha[89]        4.13    77 1.02
alpha[90]        4.58    73 1.03
alpha[91]        4.52    75 1.02
alpha[92]        4.41    75 1.02
alpha[93]        4.34    76 1.02
alpha[94]        4.04    77 1.02
alpha[95]        4.63    74 1.02
alpha[96]        4.03    78 1.02
alpha[97]        4.88    71 1.03
alpha[98]        4.71    70 1.03
alpha[99]        4.17    79 1.02
alpha[100]       3.94    78 1.02
beta[1]          2.26   533 1.01
beta[2]          0.28   122 1.02
beta[3]         -0.84   647 1.00
sd_alpha         0.32  3697 1.00
sd_year          0.24  2825 1.00
lp__       1442849.51  3942 1.00

Samples were drawn using NUTS(diag_e) at Wed Jan  6 20:42:42 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
> 
> proc.time()
   ユーザ   システム       経過  
    19.643      0.685   2031.013 
